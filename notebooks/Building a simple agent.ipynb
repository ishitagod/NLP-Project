{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87gdZNiA-g7h"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/microsoft/TextWorld/blob/main/notebooks/Building%20a%20simple%20agent.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iYrYE7H-g7k"
      },
      "source": [
        "# Training LLM to play games\n",
        "\n",
        "This tutorial outlines the steps to build an agent that learns how to play __choice-based__ text-based games generated with TextWorld."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi8oVNXe-g7m"
      },
      "source": [
        "### Prerequisite\n",
        "Install TextWorld as described in the [README.md](https://github.com/microsoft/TextWorld#readme). Most of the time, a simple `pip install` should work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "E4iHT5hP-g7n",
        "outputId": "3c4391c8-f1ed-46d6-fa15-295c6ebbb25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textworld\n",
            "  Downloading textworld-1.6.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.12/dist-packages (from textworld) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.17.1 in /usr/local/lib/python3.12/dist-packages (from textworld) (4.67.1)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from textworld) (2.0.0)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.12/dist-packages (from textworld) (3.5)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.12/dist-packages (from textworld) (10.8.0)\n",
            "Collecting tatsu==5.8.3 (from textworld)\n",
            "  Downloading TatSu-5.8.3-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting hashids>=1.2.0 (from textworld)\n",
            "  Downloading hashids-1.3.1-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting jericho>=3.3.0 (from textworld)\n",
            "  Downloading jericho-3.3.1-py3-none-any.whl.metadata (561 bytes)\n",
            "Collecting mementos>=1.3.1 (from textworld)\n",
            "  Downloading mementos-1.3.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from textworld) (3.2.0)\n",
            "Requirement already satisfied: prompt_toolkit in /usr/local/lib/python3.12/dist-packages (from textworld) (3.0.52)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->textworld) (2.23)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from jericho>=3.3.0->textworld) (3.8.11)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit->textworld) (0.2.14)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (0.20.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.3.0->textworld) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.3.0->textworld) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.3.0->textworld) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.3.0->textworld) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy>=2.1.0->jericho>=3.3.0->textworld) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy>=2.1.0->jericho>=3.3.0->textworld) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy>=2.1.0->jericho>=3.3.0->textworld) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy>=2.1.0->jericho>=3.3.0->textworld) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy>=2.1.0->jericho>=3.3.0->textworld) (2.0.1)\n",
            "Downloading textworld-1.6.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading TatSu-5.8.3-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hashids-1.3.1-py2.py3-none-any.whl (6.6 kB)\n",
            "Downloading jericho-3.3.1-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.8/325.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mementos-1.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Installing collected packages: mementos, tatsu, hashids, jericho, textworld\n",
            "Successfully installed hashids-1.3.1 jericho-3.3.1 mementos-1.3.1 tatsu-5.8.3 textworld-1.6.2\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install textworld\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWMN_YCk-g7o"
      },
      "source": [
        "and [PyTorch](https://pytorch.org/) (tested with both v1.8.2 and v.1.9.0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ItrZAyMw-g7p",
        "outputId": "2b72c183-ca15-4192-d295-8e6b0e4585c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extra"
      ],
      "metadata": {
        "id": "QGtSL09R_C9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OdNDfmiL_Ell",
        "outputId": "7477c57c-c07c-48d8-fb05-62e86dcc6435"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03kOFKtv-g7p"
      },
      "source": [
        "**[Optional]** Download all data beforehand. Otherwise, they are going to be generated as needed (slower)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0TcWNQLn-g7q",
        "outputId": "bbd4b293-c532-402c-ed21-3ee6395ea912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-24 22:30:21--  https://aka.ms/textworld/notebooks/data.zip\n",
            "Resolving aka.ms (aka.ms)... 23.211.202.11\n",
            "Connecting to aka.ms (aka.ms)|23.211.202.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://textworld.blob.core.windows.net/$web/notebooks/data.zip [following]\n",
            "--2025-11-24 22:30:22--  https://textworld.blob.core.windows.net/$web/notebooks/data.zip\n",
            "Resolving textworld.blob.core.windows.net (textworld.blob.core.windows.net)... 57.150.159.97\n",
            "Connecting to textworld.blob.core.windows.net (textworld.blob.core.windows.net)|57.150.159.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 409 Public access is not permitted on this storage account.\n",
            "2025-11-24 22:30:22 ERROR 409: Public access is not permitted on this storage account..\n",
            "\n",
            "unzip:  cannot find or open data.zip, data.zip.zip or data.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!wget https://aka.ms/textworld/notebooks/data.zip\n",
        "!unzip -nq data.zip && rm -f data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84NZBiZH-g7r"
      },
      "source": [
        "## Learning challenges\n",
        "Training an agent such that it can learn how to play text-based games is not trivial. Among other challenges, we have to deal with\n",
        "\n",
        "1. a combinatorial action space (that grows w.r.t. vocabulary)\n",
        "2. a really sparse reward signal.\n",
        "\n",
        "To ease the learning process, we will be requesting additional information alongside the game's narrative (as covered in [Playing TextWorld generated games with OpenAI Gym](Playing%20TextWorld%20generated%20games%20with%20OpenAI%20Gym.ipynb#Interact-with-the-game)). More specifically, we will request the following information:\n",
        "\n",
        "- __Description__:\n",
        "For every game state, we will get the output of the `look` command which describes the current location;\n",
        "\n",
        "- __Inventory__:\n",
        "For every game state, we will get the output of the `inventory` command which describes the player's inventory;\n",
        "\n",
        "- __Admissible commands__:\n",
        "For every game state, we will get the list of commands guaranteed to be understood by the game interpreter;\n",
        "\n",
        "- __Intermediate reward__:\n",
        "For every game state, we will get an intermediate reward which can either be:\n",
        "  - __-1__: last action needs to be undone before resuming the quest\n",
        "  -  __0__: last action didn't affect the quest\n",
        "  -  __1__: last action brought us closer to completing the quest\n",
        "\n",
        "- __Entities__:\n",
        "For every game, we will get a list of entity names that the agent can interact with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwA8xfFj-g7s"
      },
      "source": [
        "## Simple test games\n",
        "We can use TextWorld to generate a few simple games with the following handcrafted world\n",
        "```\n",
        "                     Bathroom\n",
        "                        +\n",
        "                        |\n",
        "                        +\n",
        "    Bedroom +-(d1)-+ Kitchen +--(d2)--+ Backyard\n",
        "      (P)               +                  +\n",
        "                        |                  |\n",
        "                        +                  +\n",
        "                   Living Room           Garden\n",
        "```\n",
        "where the goal is always to retrieve a hidden food item and put it on the stove which is located in the kitchen. One can lose the game if it eats the food item instead of putting it on the stove!\n",
        "\n",
        "Using `tw-make tw-simple ...`, we are going to generate the following 7 games:\n",
        "\n",
        "| gamefile | description |\n",
        "| :------- | :---------- |\n",
        "| `games/rewardsDense_goalDetailed.z8` | dense reward + detailed instructions |\n",
        "| `games/rewardsBalanced_goalDetailed.z8` | balanced rewards + detailed instructions |\n",
        "| `games/rewardsSparse_goalDetailed.z8` | sparse rewards + detailed instructions |\n",
        "| | |\n",
        "| `games/rewardsDense_goalBrief.z8` | dense rewards + no instructions but the goal is mentionned |\n",
        "| `games/rewardsBalanced_goalBrief.z8` | balanced rewards + no instructions but the goal is mentionned |\n",
        "| `games/rewardsSparse_goalBrief.z8` | sparse rewards + no instructions but the goal is mentionned |\n",
        "| | |\n",
        "| `games/rewardsSparse_goalNone.z8` | sparse rewards + no instructions/goal<br>_Hint: there's an hidden note in the game that describes the goal!_ |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5aNoPJz2-g7t"
      },
      "outputs": [],
      "source": [
        "# You can skip this if you already downloaded the games in the prequisite section.\n",
        "\n",
        "# Same as !make_games.sh\n",
        "!tw-make tw-simple --rewards dense    --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsDense_goalDetailed.z8\n",
        "!tw-make tw-simple --rewards balanced --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalDetailed.z8\n",
        "!tw-make tw-simple --rewards sparse   --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalDetailed.z8\n",
        "!tw-make tw-simple --rewards dense    --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsDense_goalBrief.z8\n",
        "!tw-make tw-simple --rewards balanced --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalBrief.z8\n",
        "!tw-make tw-simple --rewards sparse   --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalBrief.z8\n",
        "!tw-make tw-simple --rewards sparse   --goal none     --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalNone.z8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTcVVCGc-g7t"
      },
      "source": [
        "## Building the random baseline\n",
        "Let's start with building an agent that simply selects an admissible command at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9cyWYxnI-g7u"
      },
      "outputs": [],
      "source": [
        "from typing import Mapping, Any\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import textworld.gym\n",
        "\n",
        "\n",
        "class RandomAgent(textworld.gym.Agent):\n",
        "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
        "    def __init__(self, seed=1234):\n",
        "        self.seed = seed\n",
        "        self.rng = np.random.RandomState(self.seed)\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> textworld.EnvInfos:\n",
        "        return textworld.EnvInfos(admissible_commands=True)\n",
        "\n",
        "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
        "        return self.rng.choice(infos[\"admissible_commands\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttwR0osd-g7u"
      },
      "source": [
        "## Play function\n",
        "Let's write a simple play function that we will use to evaluate our agent on a given game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aJwu1_Kh-g7u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import textworld.gym\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def play(agent, path, max_step=100, nb_episodes=10, verbose=True):\n",
        "    torch.manual_seed(20211021)  # For reproducibility when using action sampling.\n",
        "\n",
        "    infos_to_request = agent.infos_to_request\n",
        "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
        "\n",
        "    gamefiles = [path]\n",
        "    if os.path.isdir(path):\n",
        "        gamefiles = glob(os.path.join(path, \"*.z8\"))\n",
        "\n",
        "    env_id = textworld.gym.register_games(gamefiles,\n",
        "                                          request_infos=infos_to_request,\n",
        "                                          max_episode_steps=max_step)\n",
        "    env = textworld.gym.make(env_id)  # Create a Gym environment to play the text game.\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            print(os.path.dirname(path), end=\"\")\n",
        "        else:\n",
        "            print(os.path.basename(path), end=\"\")\n",
        "\n",
        "    # Collect some statistics: nb_steps, final reward.\n",
        "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
        "    for no_episode in range(nb_episodes):\n",
        "        obs, infos = env.reset()  # Start new episode.\n",
        "\n",
        "        score = 0\n",
        "        done = False\n",
        "        nb_moves = 0\n",
        "        while not done:\n",
        "            command = agent.act(obs, score, done, infos)\n",
        "            obs, score, done, infos = env.step(command)\n",
        "            nb_moves += 1\n",
        "\n",
        "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
        "\n",
        "        if verbose:\n",
        "            print(\".\", end=\"\")\n",
        "        avg_moves.append(nb_moves)\n",
        "        avg_scores.append(score)\n",
        "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
        "\n",
        "    env.close()\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
        "        else:\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59ZyeTym-g7v"
      },
      "source": [
        "#### Evaluate the random agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8wdvZC7-g7v",
        "outputId": "196ff2cd-f262-4a5f-d2db-201659290e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-rewardsDense_goalDetailed.z8..........  \tavg. steps: 100.0; avg. score:  4.2 / 10.\n",
            "tw-rewardsBalanced_goalDetailed.z8..........  \tavg. steps: 100.0; avg. score:  0.7 / 4.\n",
            "tw-rewardsSparse_goalDetailed.z8..........  \tavg. steps: 100.0; avg. score:  0.0 / 1.\n"
          ]
        }
      ],
      "source": [
        "# We report the score and steps averaged over 10 playthroughs.\n",
        "play(RandomAgent(), \"./games/tw-rewardsDense_goalDetailed.z8\")    # Dense rewards\n",
        "play(RandomAgent(), \"./games/tw-rewardsBalanced_goalDetailed.z8\") # Balanced rewards\n",
        "play(RandomAgent(), \"./games/tw-rewardsSparse_goalDetailed.z8\")   # Sparse rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-497NeD-g7x"
      },
      "source": [
        "## Neural agent\n",
        "\n",
        "Now, let's create an agent that can learn to play text-based games. The agent will be trained to select a command from the list of admissible commands given the current game's narrative, inventory, and room description. Here is an overview of the architecture used for the agent:\n",
        "\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/MarcCote/TextWorld/msr_summit_2021/notebooks/figs/neural_agent.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL891ihI-g7x"
      },
      "source": [
        "### Code\n",
        "Here's the implementation of that learning agent built with [PyTorch](https://pytorch.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx2defU5-g7x",
        "outputId": "3ea66e38-4a12-4002-d6d4-20d147fc1738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:114: SyntaxWarning: invalid escape sequence '\\-'\n",
            "<>:114: SyntaxWarning: invalid escape sequence '\\-'\n",
            "/tmp/ipython-input-3165348506.py:114: SyntaxWarning: invalid escape sequence '\\-'\n",
            "  text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from typing import List, Mapping, Any, Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import textworld\n",
        "import textworld.gym\n",
        "from textworld import EnvInfos\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class CommandScorer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(CommandScorer, self).__init__()\n",
        "        torch.manual_seed(42)  # For reproducibility\n",
        "        self.embedding    = nn.Embedding(input_size, hidden_size)\n",
        "        self.encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
        "        self.cmd_encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
        "        self.state_gru    = nn.GRU(hidden_size, hidden_size)\n",
        "        self.hidden_size  = hidden_size\n",
        "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
        "        self.critic       = nn.Linear(hidden_size, 1)\n",
        "        self.att_cmd      = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "    def forward(self, obs, commands, **kwargs):\n",
        "        input_length = obs.size(0)\n",
        "        batch_size = obs.size(1)\n",
        "        nb_cmds = commands.size(1)\n",
        "\n",
        "        embedded = self.embedding(obs)\n",
        "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
        "        state_output, state_hidden = self.state_gru(encoder_hidden, self.state_hidden)\n",
        "        self.state_hidden = state_hidden\n",
        "        value = self.critic(state_output)\n",
        "\n",
        "        # Attention network over the commands.\n",
        "        cmds_embedding = self.embedding.forward(commands)\n",
        "        _, cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding)  # 1 x cmds x hidden\n",
        "\n",
        "        # Same observed state for all commands.\n",
        "        cmd_selector_input = torch.stack([state_hidden] * nb_cmds, 2)  # 1 x batch x cmds x hidden\n",
        "\n",
        "        # Same command choices for the whole batch.\n",
        "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states] * batch_size, 1)  # 1 x batch x cmds x hidden\n",
        "\n",
        "        # Concatenate the observed state and command encodings.\n",
        "        cmd_selector_input = torch.cat([cmd_selector_input, cmds_encoding_last_states], dim=-1)\n",
        "\n",
        "        # Compute one score per command.\n",
        "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1)  # 1 x Batch x cmds\n",
        "\n",
        "        probs = F.softmax(scores, dim=2)  # 1 x Batch x cmds\n",
        "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x indx\n",
        "        return scores, index, value\n",
        "\n",
        "    def reset_hidden(self, batch_size):\n",
        "        self.state_hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class NeuralAgent:\n",
        "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
        "    MAX_VOCAB_SIZE = 1000\n",
        "    UPDATE_FREQUENCY = 10\n",
        "    LOG_FREQUENCY = 1000\n",
        "    GAMMA = 0.9\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._initialized = False\n",
        "        self._epsiode_has_started = False\n",
        "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
        "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
        "\n",
        "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), 0.00003)\n",
        "\n",
        "        self.mode = \"test\"\n",
        "\n",
        "    def train(self):\n",
        "        self.mode = \"train\"\n",
        "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "        self.transitions = []\n",
        "        self.model.reset_hidden(1)\n",
        "        self.last_score = 0\n",
        "        self.no_train_step = 0\n",
        "\n",
        "    def test(self):\n",
        "        self.mode = \"test\"\n",
        "        self.model.reset_hidden(1)\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> EnvInfos:\n",
        "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
        "                        won=True, lost=True)\n",
        "\n",
        "    def _get_word_id(self, word):\n",
        "        if word not in self.word2id:\n",
        "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
        "                return self.word2id[\"<UNK>\"]\n",
        "\n",
        "            self.id2word.append(word)\n",
        "            self.word2id[word] = len(self.word2id)\n",
        "\n",
        "        return self.word2id[word]\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
        "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
        "        word_ids = list(map(self._get_word_id, text.split()))\n",
        "        return word_ids\n",
        "\n",
        "    def _process(self, texts):\n",
        "        texts = list(map(self._tokenize, texts))\n",
        "        max_len = max(len(l) for l in texts)\n",
        "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            padded[i, :len(text)] = text\n",
        "\n",
        "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
        "        padded_tensor = padded_tensor.permute(1, 0) # Batch x Seq => Seq x Batch\n",
        "        return padded_tensor\n",
        "\n",
        "    def _discount_rewards(self, last_values):\n",
        "        returns, advantages = [], []\n",
        "        R = last_values.data\n",
        "        for t in reversed(range(len(self.transitions))):\n",
        "            rewards, _, _, values = self.transitions[t]\n",
        "            R = rewards + self.GAMMA * R\n",
        "            adv = R - values\n",
        "            returns.append(R)\n",
        "            advantages.append(adv)\n",
        "\n",
        "        return returns[::-1], advantages[::-1]\n",
        "\n",
        "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
        "\n",
        "        # Build agent's observation: feedback + look + inventory.\n",
        "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
        "\n",
        "        # Tokenize and pad the input and the commands to chose from.\n",
        "        input_tensor = self._process([input_])\n",
        "        commands_tensor = self._process(infos[\"admissible_commands\"])\n",
        "\n",
        "        # Get our next action and value prediction.\n",
        "        outputs, indexes, values = self.model(input_tensor, commands_tensor)\n",
        "        action = infos[\"admissible_commands\"][indexes[0]]\n",
        "\n",
        "        if self.mode == \"test\":\n",
        "            if done:\n",
        "                self.model.reset_hidden(1)\n",
        "            return action\n",
        "\n",
        "        self.no_train_step += 1\n",
        "\n",
        "        if self.transitions:\n",
        "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
        "            self.last_score = score\n",
        "            if infos[\"won\"]:\n",
        "                reward += 100\n",
        "            if infos[\"lost\"]:\n",
        "                reward -= 100\n",
        "\n",
        "            self.transitions[-1][0] = reward  # Update reward information.\n",
        "\n",
        "        self.stats[\"max\"][\"score\"].append(score)\n",
        "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
        "            # Update model\n",
        "            returns, advantages = self._discount_rewards(values)\n",
        "\n",
        "            loss = 0\n",
        "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
        "                reward, indexes_, outputs_, values_ = transition\n",
        "\n",
        "                advantage        = advantage.detach() # Block gradients flow here.\n",
        "                probs            = F.softmax(outputs_, dim=2)\n",
        "                log_probs        = torch.log(probs)\n",
        "                log_action_probs = log_probs.gather(2, indexes_)\n",
        "                policy_loss      = (-log_action_probs * advantage).sum()\n",
        "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
        "                entropy     = (-probs * log_probs).sum()\n",
        "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
        "\n",
        "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
        "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
        "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
        "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
        "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
        "\n",
        "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
        "                msg = \"{:6d}. \".format(self.no_train_step)\n",
        "                msg += \"  \".join(\"{}: {: 3.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
        "                msg += \"  \" + \"  \".join(\"{}: {:2d}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
        "                msg += \"  vocab: {:3d}\".format(len(self.id2word))\n",
        "                print(msg)\n",
        "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            self.transitions = []\n",
        "            self.model.reset_hidden(1)\n",
        "        else:\n",
        "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
        "            self.transitions.append([None, indexes, outputs, values])  # Reward will be set on the next call\n",
        "\n",
        "        if done:\n",
        "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
        "\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adNG96Ig-g7z"
      },
      "source": [
        "### Training the neural agent\n",
        "Let's first evaluate the agent before training to get a sense of its initial performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj14oGle-g7z",
        "outputId": "b049525b-c76d-4571-b4dd-064e2a5eb467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-rewardsDense_goalDetailed.z8..........  \tavg. steps: 100.0; avg. score:  4.3 / 10.\n"
          ]
        }
      ],
      "source": [
        "agent = NeuralAgent()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLZF0Cwt-g70"
      },
      "source": [
        "Unsurprisingly, the result is not much different from what the random agent can achieve since our neural agent is initialized to a random policy.\n",
        "\n",
        "Let's train the agent for a few episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "D8AYQpGb-g70",
        "outputId": "12a303ad-cf40-4c8f-fc4b-cef48d6576d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "  1000. reward:  0.042  policy:  0.258  value:  0.074  entropy:  2.337  confidence:  0.099  score:  8  vocab: 258\n",
            "  2000. reward: -0.058  policy: -1.414  value:  24.997  entropy:  2.385  confidence:  0.095  score:  9  vocab: 316\n",
            "  3000. reward:  0.052  policy:  0.146  value:  0.097  entropy:  2.429  confidence:  0.092  score:  9  vocab: 318\n",
            "  4000. reward:  0.042  policy: -0.040  value:  0.081  entropy:  2.397  confidence:  0.095  score:  5  vocab: 318\n",
            "  5000. reward:  0.053  policy:  0.084  value:  0.104  entropy:  2.480  confidence:  0.089  score:  7  vocab: 319\n",
            "  6000. reward:  0.048  policy: -0.015  value:  0.083  entropy:  2.402  confidence:  0.095  score:  5  vocab: 319\n",
            "  7000. reward:  0.053  policy:  0.030  value:  0.086  entropy:  2.426  confidence:  0.096  score:  8  vocab: 319\n",
            "  8000. reward: -0.049  policy: -0.944  value:  18.233  entropy:  2.421  confidence:  0.098  score:  9  vocab: 320\n",
            "  9000. reward:  0.054  policy:  0.062  value:  0.093  entropy:  2.477  confidence:  0.093  score:  6  vocab: 320\n",
            " 10000. reward: -0.056  policy: -1.178  value:  20.702  entropy:  2.374  confidence:  0.103  score:  9  vocab: 321\n",
            " 11000. reward:  0.054  policy:  0.010  value:  0.105  entropy:  2.428  confidence:  0.097  score:  6  vocab: 321\n",
            " 12000. reward:  0.066  policy:  0.114  value:  0.131  entropy:  2.443  confidence:  0.096  score:  9  vocab: 321\n",
            " 13000. reward:  0.062  policy: -0.002  value:  0.107  entropy:  2.487  confidence:  0.097  score:  7  vocab: 321\n",
            " 14000. reward:  0.054  policy: -0.051  value:  0.091  entropy:  2.382  confidence:  0.102  score:  7  vocab: 321\n",
            " 15000. reward:  0.062  policy:  0.042  value:  0.121  entropy:  2.476  confidence:  0.098  score:  9  vocab: 321\n",
            " 16000. reward:  0.052  policy: -0.046  value:  0.088  entropy:  2.414  confidence:  0.099  score:  7  vocab: 321\n",
            " 17000. reward:  0.063  policy:  0.039  value:  0.112  entropy:  2.420  confidence:  0.101  score:  7  vocab: 321\n",
            " 18000. reward:  0.057  policy: -0.036  value:  0.094  entropy:  2.432  confidence:  0.098  score:  7  vocab: 321\n",
            " 19000. reward:  0.060  policy: -0.008  value:  0.091  entropy:  2.438  confidence:  0.102  score:  7  vocab: 321\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-873100528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Tell the agent it should update its parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./games/tw-rewardsDense_goalDetailed.z8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Dense rewards game.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained in {:.2f} secs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstarttime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1094139752.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(agent, path, max_step, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mnb_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mnb_moves\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3165348506.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, score, done, infos)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# You can skip this if you already downloaded the data in the prequisite section.\n",
        "\n",
        "from time import time\n",
        "agent = NeuralAgent()\n",
        "\n",
        "print(\"Training\")\n",
        "agent.train()  # Tell the agent it should update its parameters.\n",
        "starttime = time()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\", nb_episodes=500, verbose=False)  # Dense rewards game.\n",
        "\n",
        "print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
        "\n",
        "# Save the trained agent.\n",
        "import os\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "torch.save(agent, 'checkpoints/agent_trained_on_single_game.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVWcX02u-g71"
      },
      "source": [
        "#### Testing the agent trained on a single game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JWhxtva-g71"
      },
      "outputs": [],
      "source": [
        "# We report the score and steps averaged over 10 playthroughs.\n",
        "agent = torch.load('checkpoints/agent_trained_on_single_game.pt')\n",
        "agent.test()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Dense rewards game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NgN3uYr-g72"
      },
      "source": [
        "Of course, since we trained on that single simple game, it's not surprinsing the agent can achieve a high score on it. It would be more interesting to evaluate the generalization capability of the agent.\n",
        "\n",
        "To do so, we are going to test the agent on another game drawn from the same game distribution (i.e. same world but the goal is to pick another food item). Let's generate `games/another_game.z8` with the same rewards density (`--rewards dense`) and the same goal description (`--goal detailed`), but using `--seed 1` and without the `--test` flag (to make sure the game is not part of the test set since `games/rewardsDense_goalDetailed.z8` is)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBZfWVKg-g72",
        "outputId": "31c7a504-e8ab-411a-afa6-c81c85bc508c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed: 1\n",
            "Game generated: /content/games/tw-another_game.z8\n",
            "\n",
            "Objective:\n",
            "Hey, thanks for coming over to the TextWorld today, there is something I need you to do for me. First of all, you could, like, look and see that the antique trunk inside the bedroom is opened. Then, recover the old key from the antique trunk. Then, make absolutely sure that the wooden door inside the bedroom is unlocked. After unlocking the wooden door, open the wooden door in the bedroom. Then, try to head east. After that, try to travel south. Once you get through with that, take the milk from the couch within the living room. Having taken the milk, attempt to travel north. That done, rest the milk on the stove inside the kitchen. And if you do that, you're the winner!\n",
            "\n",
            "Walkthrough:\n",
            "open antique trunk > take old key from antique trunk > unlock wooden door with old key > open wooden door > go east > go south > take milk from couch > go north > put milk on stove\n",
            "\n",
            "-= Stats =-\n",
            "Nb. locations: 6\n",
            "Nb. objects: 28\n"
          ]
        }
      ],
      "source": [
        "!tw-make tw-simple --rewards dense --goal detailed --seed 1 --output games/tw-another_game.z8 -v -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eChciJY_-g73",
        "outputId": "670bbca0-1d07-4718-d49c-58d003c9a38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-another_game.z8..........  \tavg. steps: 100.0; avg. score:  3.9 / 8.\n",
            "tw-another_game.z8...... 49000. reward:  0.070  policy:  0.009  value:  0.163  entropy:  2.408  confidence:  0.125  score:  8  vocab: 766\n",
            "....  \tavg. steps:  95.3; avg. score:  6.6 / 8.\n"
          ]
        }
      ],
      "source": [
        "# We report the score and steps averaged over 10 playthroughs.\n",
        "play(RandomAgent(), \"./games/tw-another_game.z8\")\n",
        "play(agent, \"./games/tw-another_game.z8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZX8YhBA-g73"
      },
      "source": [
        "As we can see the trained agent does a bit better than the random agent. In order to improve the agent's generalization capability, we should train it on many different games drawn from the game distribution.\n",
        "\n",
        "One could use the following command to easily generate 100 training games:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rL_r42_-g73",
        "outputId": "33998075-a25f-4152-f2fb-31d7612a2836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed: 4\n",
            "Global seed: 2\n",
            "Global seed: 3\n",
            "Global seed: 1\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-ek06H8B7uqoYFVEy.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-E5eLHkaXFk6BSgR1.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-D8gMTlO8cPoEtgZx.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-ekDZtbGXIbO5FKp8.z8\n",
            "Global seed: 5\n",
            "Global seed: 6\n",
            "Global seed: 7\n",
            "Global seed: 8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-KJODI168SvJVFM9x.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-o2RVTmrEi6R5T3p0.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-7KpYUDDdckE0cBqZ.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-68kvf8x7TBd9Iq0P.z8\n",
            "Global seed: 10\n",
            "Global seed: 9\n",
            "Global seed: 11\n",
            "Global seed: 12\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-redEHVr6CmKYhrJg.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Q9nDu630U5j3tqBG.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Qbq3h3VWFkPdtogB.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-NPQ8TkJ9i2x6fYDM.z8\n",
            "Global seed: 13\n",
            "Global seed: 14\n",
            "Global seed: 15\n",
            "Global seed: 16\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1QKVfg6YhRb1ul1e.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-jROVIEqEIya6Tr0L.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6GMVtjVYF5QRupyN.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-7yGrcV9pTE8DF75n.z8\n",
            "Global seed: 17\n",
            "Global seed: 18\n",
            "Global seed: 20\n",
            "Global seed: 19\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-nrEoHEqgUba7U1nx.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-o8WVtobyuRR0Uqv5.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-VLpEiW2msKJpTZ7J.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Mn8oTkr2fvv8TX1.z8\n",
            "Global seed: 21\n",
            "Global seed: 22\n",
            "Global seed: 23\n",
            "Global seed: 24\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-M7MmCGG5i6kES1kV.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-JGJdTBvVHrpBiVy5.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6GZ3CqJvCX9rfev3.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1MB8fmosEL9HNEv.z8\n",
            "Global seed: 25\n",
            "Global seed: 26\n",
            "Global seed: 27\n",
            "Global seed: 28\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-mn3JuMrnfPNNI5K5.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-pGedtxVJsYDxsGaV.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-P6RdC912uMrbcXBX.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-R2bxU9dJUK0LCxk0.z8\n",
            "Global seed: 29\n",
            "Global seed: 30\n",
            "Global seed: 31\n",
            "Global seed: 32\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6WNBiZyofr8mu6MG.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-W1qJibX5FqLRS3kL.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-eDi7Z2iEJ7FdL9.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-QlDlS6DxCMnVu81D.z8\n",
            "Global seed: 33\n",
            "Global seed: 34\n",
            "Global seed: 35\n",
            "Global seed: 36\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-dZ2oU2KnflPksnEY.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-5rjrFkZEiyOIj1y.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-KrNpTrdMtdqVUKOB.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-J9ylcQmmc190Cgn3.z8\n",
            "Global seed: 37\n",
            "Global seed: 38\n",
            "Global seed: 39\n",
            "Global seed: 40\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-M8xnUOBkulX7HNQX.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-535aCDxgu5MqF7R1.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-mDjNimx7S5a5tmZ7.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-nOVQCNlrINVxIDje.z8\n",
            "Global seed: 41\n",
            "Global seed: 42\n",
            "Global seed: 43\n",
            "Global seed: 44\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-566vUvgjfXgU9GK.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-67XKC3EyH2riOk8.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-El9oUXJWIYVgF1jy.z8\n",
            "Global seed: 45\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-mbJLuQBoCbbkUv1n.z8\n",
            "Global seed: 46\n",
            "Global seed: 47\n",
            "Global seed: 48\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-M2vecROLFVBxHBvl.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-mNB2tM2ohGmRIB2J.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-JBP7TQZ6fV7biZ9q.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-aernSYY1f7rrflvB.z8\n",
            "Global seed: 50\n",
            "Global seed: 49\n",
            "Global seed: 51\n",
            "Global seed: 52\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-5VVVSK6fGjlspnj.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Oq9ns8K0uK5EhJvr.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Mj8NTxYWso7LfmN9.z8\n",
            "Global seed: 53\n",
            "Global seed: 54\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-xqa9SlmxsDmVs8yb.z8\n",
            "Global seed: 55\n",
            "Global seed: 56\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-GN1YHrQ6s6vBTnNb.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-EloyS7BZs8LRHNVn.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-XR5RUnmMIgOnCYyy.z8\n",
            "Global seed: 57\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-p1RLFX7MU7KjFy0d.z8\n",
            "Global seed: 58\n",
            "Global seed: 59\n",
            "Global seed: 60\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-WxnkhG07upKRhbNV.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Kx32iybbtDGRFQl6.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-DWvMiBQvCR5sNkx.z8\n",
            "Global seed: 61\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-R7y2UJkBUeEqS8Bk.z8\n",
            "Global seed: 62\n",
            "Global seed: 63\n",
            "Global seed: 64\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-YnvLs6vYIWvWC51e.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1EQPheDOiVB8I7m5.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6VXXCVGKiEWkhPer.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-G6a7HeZPcJNGhLQ2.z8\n",
            "Global seed: 65\n",
            "Global seed: 66\n",
            "Global seed: 67\n",
            "Global seed: 68\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1jr7FeK9TgZCQ9n.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-0a72tMGVtQnPSvMR.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-2OOjSlaNUBKcgLy.z8\n",
            "Global seed: 69\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1vbrCekqT6xMcMdP.z8\n",
            "Global seed: 70\n",
            "Global seed: 71\n",
            "Global seed: 72\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-kGDLudo8fXeJH1Yg.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6kRniM7gfXQNCyj6.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-RvQYhbdKfMyqS5Wp.z8\n",
            "Global seed: 73\n",
            "Global seed: 74\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-aMq9U18aFMlOHVEe.z8\n",
            "Global seed: 75\n",
            "Global seed: 76\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-8koWh6KqcgNPI8Ox.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-165ai83atZWatMRa.z8\n",
            "Global seed: 77\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Eykru3m8ilOGiBnx.z8\n",
            "Global seed: 78\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-vKm0IdN8c0VRfJl1.z8\n",
            "Global seed: 79\n",
            "Global seed: 80\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-D3PEFjpNIQykuqra.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-gPyQi5vkhdQOUV0J.z8\n",
            "Global seed: 81\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-jYPJukgls6oZf6qG.z8\n",
            "Global seed: 82\n",
            "Global seed: 83\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-eRNBC9q7tr5Vi3Mv.z8\n",
            "Global seed: 84\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-0lvotxEKtYZ6umyJ.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-3bj9creDs3V0IKKN.z8\n",
            "Global seed: 85\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-qZDfWDNTJmQiLvq.z8\n",
            "Global seed: 86\n",
            "Global seed: 87\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6yPJsEJMTeR6fpPR.z8\n",
            "Global seed: 88\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-WrmvTdgGc05rh3eZ.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-g0GEcMQ5CWaeF3nJ.z8\n",
            "Global seed: 89\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-5PBeh9J3IX5XUjL3.z8\n",
            "Global seed: 90\n",
            "Global seed: 91\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-DvPBs6OphW6nCnQ3.z8\n",
            "Global seed: 92\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-eBxGu3KeiYjbHDl9.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-XElPhYRjT8gWs1eM.z8\n",
            "Global seed: 93\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-3qLvIOZ9c66Igby.z8\n",
            "Global seed: 94\n",
            "Global seed: 95\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-OWRZsvM8h6XyIpQy.z8\n",
            "Global seed: 96\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-gkdXTMLPtKpeSY5J.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-n3OYtnV7IY8rTkBq.z8\n",
            "Global seed: 97\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-q3rdiVK5I1lrsNpj.z8\n",
            "Global seed: 98\n",
            "Global seed: 99\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-WvgBsoOmhDObfRgL.z8\n",
            "Global seed: 100\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-xQNoFqlDs10nfr79.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1dgPTd1ki5kQUkrn.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-o7PNc5j0URa5HYLb.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-2KK5iXD8ueb3ikl6.z8\n"
          ]
        }
      ],
      "source": [
        "# You can skip this if you already downloaded the data in the prequisite section.\n",
        "\n",
        "! seq 1 100 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --format z8 --output training_games/ --seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZaRUNfp-g74"
      },
      "source": [
        "Then, we train our agent on that set of training games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC64MOKm-g75",
        "outputId": "9ae70cb4-83c4-44b1-ab68-085fd71cbffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 100 games\n",
            "  1000. reward:  0.050  policy:  0.377  value:  0.121  entropy:  2.379  confidence:  0.096  score:  9  vocab: 461\n",
            "  2000. reward:  0.041  policy:  0.057  value:  0.074  entropy:  2.395  confidence:  0.093  score:  6  vocab: 531\n",
            "  3000. reward: -0.063  policy: -1.431  value:  20.784  entropy:  2.419  confidence:  0.092  score:  6  vocab: 584\n",
            "  4000. reward:  0.050  policy:  0.103  value:  0.086  entropy:  2.435  confidence:  0.092  score:  6  vocab: 612\n",
            "  5000. reward:  0.051  policy:  0.116  value:  0.093  entropy:  2.431  confidence:  0.092  score:  6  vocab: 630\n",
            "  6000. reward:  0.041  policy: -0.021  value:  0.077  entropy:  2.399  confidence:  0.094  score:  5  vocab: 643\n",
            "  7000. reward:  0.051  policy:  0.093  value:  0.088  entropy:  2.382  confidence:  0.096  score:  7  vocab: 662\n",
            "  8000. reward: -0.276  policy: -3.072  value:  48.306  entropy:  2.455  confidence:  0.092  score:  7  vocab: 672\n",
            "  9000. reward:  0.154  policy:  1.631  value:  19.768  entropy:  2.361  confidence:  0.099  score:  7  vocab: 676\n",
            " 10000. reward:  0.039  policy: -0.063  value:  0.071  entropy:  2.367  confidence:  0.100  score:  6  vocab: 690\n",
            " 11000. reward:  0.060  policy:  0.153  value:  0.101  entropy:  2.471  confidence:  0.093  score:  7  vocab: 691\n",
            " 12000. reward:  0.044  policy: -0.026  value:  0.075  entropy:  2.384  confidence:  0.101  score:  6  vocab: 693\n",
            " 13000. reward: -0.048  policy: -0.707  value:  13.539  entropy:  2.448  confidence:  0.093  score:  7  vocab: 695\n",
            " 14000. reward:  0.052  policy:  0.038  value:  0.073  entropy:  2.404  confidence:  0.098  score:  8  vocab: 698\n",
            " 15000. reward: -0.053  policy: -1.578  value:  22.615  entropy:  2.409  confidence:  0.101  score:  6  vocab: 699\n",
            " 16000. reward:  0.176  policy:  1.735  value:  23.434  entropy:  2.433  confidence:  0.098  score:  7  vocab: 709\n",
            " 17000. reward:  0.056  policy: -0.045  value:  0.096  entropy:  2.411  confidence:  0.102  score:  9  vocab: 709\n",
            " 18000. reward:  0.060  policy:  0.035  value:  0.102  entropy:  2.453  confidence:  0.095  score:  8  vocab: 709\n",
            " 19000. reward:  0.058  policy: -0.013  value:  0.114  entropy:  2.500  confidence:  0.095  score:  7  vocab: 713\n",
            " 20000. reward:  0.060  policy:  0.037  value:  0.103  entropy:  2.440  confidence:  0.101  score:  7  vocab: 714\n",
            " 21000. reward: -0.039  policy: -0.264  value:  5.371  entropy:  2.532  confidence:  0.094  score:  9  vocab: 718\n",
            " 22000. reward: -0.154  policy: -3.744  value:  48.503  entropy:  2.410  confidence:  0.101  score:  7  vocab: 720\n",
            " 23000. reward: -0.037  policy: -1.940  value:  25.010  entropy:  2.501  confidence:  0.098  score:  9  vocab: 721\n",
            " 24000. reward: -0.150  policy: -2.416  value:  41.277  entropy:  2.490  confidence:  0.098  score:  9  vocab: 728\n",
            " 25000. reward:  0.063  policy: -0.017  value:  0.120  entropy:  2.492  confidence:  0.097  score:  9  vocab: 729\n",
            " 26000. reward: -0.041  policy: -1.402  value:  21.583  entropy:  2.498  confidence:  0.101  score:  8  vocab: 731\n",
            " 27000. reward: -0.256  policy: -3.694  value:  51.176  entropy:  2.423  confidence:  0.106  score:  7  vocab: 732\n",
            " 28000. reward:  0.074  policy: -0.314  value:  39.795  entropy:  2.490  confidence:  0.104  score:  9  vocab: 733\n",
            " 29000. reward:  0.294  policy:  1.165  value:  16.267  entropy:  2.419  confidence:  0.114  score:  8  vocab: 736\n",
            " 30000. reward:  0.412  policy:  3.767  value:  57.960  entropy:  2.441  confidence:  0.113  score: 10  vocab: 740\n",
            " 31000. reward: -0.354  policy: -4.831  value:  71.141  entropy:  2.459  confidence:  0.116  score:  9  vocab: 744\n",
            " 32000. reward: -0.026  policy: -1.192  value:  22.539  entropy:  2.451  confidence:  0.123  score:  9  vocab: 745\n",
            " 33000. reward:  0.078  policy:  0.252  value:  44.224  entropy:  2.438  confidence:  0.124  score: 10  vocab: 746\n",
            " 34000. reward:  0.091  policy:  0.061  value:  48.789  entropy:  2.438  confidence:  0.124  score: 10  vocab: 747\n",
            " 35000. reward: -0.032  policy: -1.223  value:  20.593  entropy:  2.457  confidence:  0.111  score:  9  vocab: 748\n",
            " 36000. reward:  0.070  policy: -0.111  value:  0.178  entropy:  2.458  confidence:  0.115  score:  9  vocab: 748\n",
            " 37000. reward:  0.082  policy:  0.097  value:  0.158  entropy:  2.502  confidence:  0.112  score:  9  vocab: 750\n",
            " 38000. reward: -0.036  policy: -0.332  value:  5.491  entropy:  2.385  confidence:  0.129  score:  9  vocab: 753\n",
            " 39000. reward: -0.139  policy: -2.182  value:  57.491  entropy:  2.449  confidence:  0.118  score:  9  vocab: 755\n",
            " 40000. reward: -0.254  policy: -4.199  value:  62.734  entropy:  2.476  confidence:  0.111  score:  9  vocab: 757\n",
            " 41000. reward:  0.080  policy:  0.092  value:  0.162  entropy:  2.566  confidence:  0.108  score:  9  vocab: 757\n",
            " 42000. reward:  0.068  policy: -0.067  value:  0.166  entropy:  2.497  confidence:  0.116  score:  9  vocab: 757\n",
            " 43000. reward: -0.561  policy: -4.762  value:  75.892  entropy:  2.513  confidence:  0.117  score:  9  vocab: 760\n",
            " 44000. reward:  0.071  policy: -0.038  value:  0.160  entropy:  2.436  confidence:  0.120  score:  9  vocab: 760\n",
            " 45000. reward: -0.346  policy: -2.694  value:  101.688  entropy:  2.421  confidence:  0.121  score:  9  vocab: 761\n",
            " 46000. reward: -0.349  policy: -5.029  value:  70.893  entropy:  2.447  confidence:  0.125  score:  9  vocab: 764\n",
            " 47000. reward: -0.136  policy: -2.101  value:  34.896  entropy:  2.358  confidence:  0.135  score:  9  vocab: 766\n",
            " 48000. reward: -0.036  policy: -1.420  value:  22.433  entropy:  2.454  confidence:  0.119  score:  9  vocab: 766\n",
            "Trained in 2534.61 secs\n"
          ]
        }
      ],
      "source": [
        "# You can skip this if you already downloaded the data in the prequisite section.\n",
        "\n",
        "from time import time\n",
        "agent = NeuralAgent()\n",
        "\n",
        "print(\"Training on 100 games\")\n",
        "agent.train()  # Tell the agent it should update its parameters.\n",
        "starttime = time()\n",
        "play(agent, \"./training_games/\", nb_episodes=100 * 5, verbose=False)  # Each game will be seen 5 times.\n",
        "print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
        "\n",
        "# Save the trained agent.\n",
        "import os\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "torch.save(agent, 'checkpoints/agent_trained_on_multiple_games.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emP9RlJN-g75"
      },
      "source": [
        "#### Testing the agent trained on 100 games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dv5tW-2-g76",
        "outputId": "f75b6721-cd12-4356-ab25-40f1ec4e091f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-rewardsDense_goalDetailed.z8..........  \tavg. steps:  79.2; avg. score:  9.0 / 10.\n",
            "tw-another_game.z8..........  \tavg. steps:  90.8; avg. score:  6.5 / 8.\n"
          ]
        }
      ],
      "source": [
        "agent = torch.load('checkpoints/agent_trained_on_multiple_games.pt', weights_only = False)\n",
        "agent.test()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Averaged over 10 playthroughs.\n",
        "play(agent, \"./games/tw-another_game.z8\")               # Averaged over 10 playthroughs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ESnjRC5-g76"
      },
      "source": [
        "Compare it to the agent trained on a single game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zO4sIsxx-g76"
      },
      "outputs": [],
      "source": [
        "agent = torch.load('checkpoints/agent_trained_on_single_game.pt')\n",
        "agent.test()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Averaged over 10 playthroughs.\n",
        "play(agent, \"./games/tw-another_game.z8\")               # Averaged over 10 playthroughs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an8sgS5u-g77"
      },
      "source": [
        "#### Evaluating the agent on a test distribution\n",
        "We will generate 20 test games and evaluate the agent on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZA10_rH-g77",
        "outputId": "edb4937d-e21a-4bb9-b3a9-986757aced65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xargs: tw-make: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# You can skip this if you already downloaded the games in the prequisite section.\n",
        "\n",
        "! seq 1 20 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --test --format z8 --output testing_games/ --seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmpEXkij-g78"
      },
      "outputs": [],
      "source": [
        "agent = torch.load('checkpoints/agent_trained_on_multiple_games.pt')\n",
        "agent.test()\n",
        "play(RandomAgent(), \"./testing_games/\", nb_episodes=20 * 10)\n",
        "play(agent, \"./testing_games/\", nb_episodes=20 * 10)  # Averaged over 10 playthroughs for each test game."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying using an LLM DistilBert/GPT2"
      ],
      "metadata": {
        "id": "ffzSULJmAHzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dgSeTmaLANlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# llm_enhanced_agent.py\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from typing import List, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------------------\n",
        "# Model: DistilBERT encoder + dot-product scorer\n",
        "# ---------------------------\n",
        "class TransformerCommandScorer(nn.Module):\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased\", pooling=\"cls\", freeze_encoder=False):\n",
        "        \"\"\"\n",
        "        pooling: 'cls' or 'mean' pooling of token embeddings\n",
        "        freeze_encoder: if True, keep encoder frozen (train only head)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.hidden_size = self.encoder.config.hidden_size\n",
        "        self.pooling = pooling\n",
        "        if freeze_encoder:\n",
        "            for p in self.encoder.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # small projection optional (helps if you want to use LoRA or adapter later)\n",
        "        # We'll keep identity (no projection) for now.\n",
        "        # If you want, add: self.proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        # For now:\n",
        "        self.proj = None\n",
        "\n",
        "    def encode_text(self, input_ids, attention_mask):\n",
        "        # input_ids: (batch, seq_len)\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        last = out.last_hidden_state  # (batch, seq_len, hidden)\n",
        "        if self.pooling == \"cls\":\n",
        "            # DistilBERT doesn't have pooled output; use first token\n",
        "            emb = last[:, 0, :]  # (batch, hidden)\n",
        "        else:\n",
        "            # mean pooling over tokens (mask aware)\n",
        "            mask = attention_mask.unsqueeze(-1)  # (batch, seq_len,1)\n",
        "            sum_vec = (last * mask).sum(1)\n",
        "            denom = mask.sum(1).clamp(min=1e-9)\n",
        "            emb = sum_vec / denom  # (batch, hidden)\n",
        "        if self.proj is not None:\n",
        "            emb = self.proj(emb)\n",
        "        return emb  # (batch, hidden)\n",
        "\n",
        "    def forward(self, obs_inputs, obs_masks, cmds_inputs, cmds_masks, cmds_counts):\n",
        "        \"\"\"\n",
        "        obs_inputs: (batch, obs_len)\n",
        "        obs_masks: (batch, obs_len)\n",
        "        cmds_inputs: (batch, max_cmds, cmd_len)\n",
        "        cmds_masks: (batch, max_cmds, cmd_len)\n",
        "        cmds_counts: (batch,) number of valid commands per sample\n",
        "        Returns logits: (batch, max_cmds) - invalid padded cmd positions set to -inf later\n",
        "        \"\"\"\n",
        "        bsz = obs_inputs.size(0)\n",
        "        max_cmds = cmds_inputs.size(1)\n",
        "        # encode obs\n",
        "        obs_emb = self.encode_text(obs_inputs, obs_masks)  # (b, hidden)\n",
        "        # flatten commands to encode in batch for efficiency\n",
        "        cmds_flat = cmds_inputs.view(bsz * max_cmds, -1)\n",
        "        masks_flat = cmds_masks.view(bsz * max_cmds, -1)\n",
        "        # encode\n",
        "        cmds_emb_flat = self.encode_text(cmds_flat, masks_flat)  # (b*max_cmds, hidden)\n",
        "        cmds_emb = cmds_emb_flat.view(bsz, max_cmds, -1)  # (b, max_cmds, hidden)\n",
        "\n",
        "        # compute dot product: logits[b, j] = obs_emb[b] dot cmds_emb[b, j]\n",
        "        # obs_emb.unsqueeze(1): (b,1,h)\n",
        "        logits = torch.bmm(cmds_emb, obs_emb.unsqueeze(-1)).squeeze(-1)  # (b, max_cmds)\n",
        "        # mask out padded commands with very negative number (we will provide mask later)\n",
        "        return logits  # (b, max_cmds)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset & collate_fn for walkthrough dataset JSONL\n",
        "# Each example: { \"observation\": \"...\", \"inventory\": \"...\", \"admissible_commands\": [...], \"gold_action\": \"take key\" }\n",
        "# ---------------------------\n",
        "class WalkthroughDataset(Dataset):\n",
        "    def __init__(self, jsonl_path, tokenizer: AutoTokenizer, max_obs_len=256, max_cmd_len=32):\n",
        "        self.examples = []\n",
        "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for ln in f:\n",
        "                j = json.loads(ln)\n",
        "                # make canonical input text\n",
        "                obs = j.get(\"observation\", \"\")\n",
        "                inv = j.get(\"inventory\", \"\")\n",
        "                obs_text = obs.strip()\n",
        "                if inv:\n",
        "                    obs_text += \"\\nInventory: \" + inv.strip()\n",
        "                adm = j.get(\"admissible_commands\", []) or []\n",
        "                gold = j.get(\"gold_action\", \"\").strip()\n",
        "                # If gold not in admissible commands, skip (or handle mapping)\n",
        "                if gold not in adm:\n",
        "                    # skip sample if gold not in admissible list\n",
        "                    continue\n",
        "                self.examples.append({\"obs_text\": obs_text, \"admissible\": adm, \"gold\": gold})\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_obs_len = max_obs_len\n",
        "        self.max_cmd_len = max_cmd_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx]\n",
        "        return ex  # we'll tokenize in collate_fn\n",
        "\n",
        "\n",
        "def collate_fn(batch, tokenizer, max_obs_len=256, max_cmd_len=32):\n",
        "    \"\"\"\n",
        "    batch: list of examples: {\"obs_text\",\"admissible\", \"gold\"}\n",
        "    returns dict of tensors:\n",
        "      obs_input_ids, obs_attention_mask,\n",
        "      cmds_input_ids (b, max_cmds, cmd_len), cmds_attention_mask,\n",
        "      cmds_counts (b,), labels (b,) - index of gold command in the adm list\n",
        "    \"\"\"\n",
        "    bsz = len(batch)\n",
        "    # gather counts\n",
        "    cmds_counts = [len(x[\"admissible\"]) for x in batch]\n",
        "    max_cmds = max(cmds_counts)\n",
        "    # tokenise observations in batch\n",
        "    obs_texts = [x[\"obs_text\"] for x in batch]\n",
        "    obs_tok = tokenizer(obs_texts, padding=True, truncation=True, max_length=max_obs_len, return_tensors=\"pt\")\n",
        "    # prepare cmds: flatten list-of-lists, pad to max_cmds\n",
        "    all_cmds = []\n",
        "    for x in batch:\n",
        "        # pad with empty string if fewer commands\n",
        "        cmd_list = x[\"admissible\"] + [\"\"] * (max_cmds - len(x[\"admissible\"]))\n",
        "        all_cmds.extend(cmd_list)\n",
        "    cmd_tok = tokenizer(all_cmds, padding=True, truncation=True, max_length=max_cmd_len, return_tensors=\"pt\")\n",
        "    # reshape to (b, max_cmds, cmd_len)\n",
        "    cmd_input_ids = cmd_tok[\"input_ids\"].view(bsz, max_cmds, -1)\n",
        "    cmd_attention_mask = cmd_tok[\"attention_mask\"].view(bsz, max_cmds, -1)\n",
        "\n",
        "    # compute labels: index of gold in adm list\n",
        "    labels = []\n",
        "    for i, x in enumerate(batch):\n",
        "        labels.append(x[\"admissible\"].index(x[\"gold\"]))\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        \"obs_input_ids\": obs_tok[\"input_ids\"].to(device),\n",
        "        \"obs_attention_mask\": obs_tok[\"attention_mask\"].to(device),\n",
        "        \"cmds_input_ids\": cmd_input_ids.to(device),\n",
        "        \"cmds_attention_mask\": cmd_attention_mask.to(device),\n",
        "        \"cmds_counts\": torch.tensor(cmds_counts, dtype=torch.long, device=device),\n",
        "        \"labels\": labels.to(device)\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Training loop\n",
        "# ---------------------------\n",
        "def train_supervised(jsonl_path, epochs=3, batch_size=8, lr=2e-5, model_name=\"distilbert-base-uncased\", max_obs_len=256, max_cmd_len=32):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = TransformerCommandScorer(model_name=model_name, pooling=\"cls\").to(device)\n",
        "\n",
        "    dataset = WalkthroughDataset(jsonl_path, tokenizer, max_obs_len=max_obs_len, max_cmd_len=max_cmd_len)\n",
        "    print(\"Loaded dataset size:\", len(dataset))\n",
        "    collate = lambda batch: collate_fn(batch, tokenizer, max_obs_len, max_cmd_len)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "\n",
        "    # optimizer & scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    total_steps = epochs * len(loader)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=math.ceil(0.06 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        total_loss = 0.0\n",
        "        total_acc = 0.0\n",
        "        n_batches = 0\n",
        "        for batch in loader:\n",
        "            logits = model(batch[\"obs_input_ids\"], batch[\"obs_attention_mask\"],\n",
        "                           batch[\"cmds_input_ids\"], batch[\"cmds_attention_mask\"], batch[\"cmds_counts\"])\n",
        "            # mask out invalid commands (we padded extras with empty string)\n",
        "            # Create mask where cmds_counts < max_cmds\n",
        "            max_cmds = logits.size(1)\n",
        "            mask = torch.arange(max_cmds, device=device).unsqueeze(0) >= batch[\"cmds_counts\"].unsqueeze(1)\n",
        "            logits = logits.masked_fill(mask, -1e9)  # set padded positions to -inf\n",
        "\n",
        "            labels = batch[\"labels\"]\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = logits.argmax(dim=1)\n",
        "            total_acc += (preds == labels).float().mean().item()\n",
        "            n_batches += 1\n",
        "\n",
        "        print(f\"Epoch {epoch} - loss: {total_loss / n_batches:.4f}, acc: {total_acc / n_batches:.4f}\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Inference helper to plug into play() routine\n",
        "# ---------------------------\n",
        "def select_action_from_model(model: TransformerCommandScorer, tokenizer, obs_text: str, admissible_commands: List[str], max_obs_len=256, max_cmd_len=32):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # prepare obs\n",
        "        obs_tok = tokenizer([obs_text], padding=True, truncation=True, max_length=max_obs_len, return_tensors=\"pt\")\n",
        "        # prepare commands\n",
        "        cmds = admissible_commands\n",
        "        cmds_tok = tokenizer(cmds, padding=True, truncation=True, max_length=max_cmd_len, return_tensors=\"pt\")\n",
        "        # expand dims to create batch-size 1 and max_cmds\n",
        "        b_obs_input_ids = obs_tok[\"input_ids\"].to(device)\n",
        "        b_obs_attention_mask = obs_tok[\"attention_mask\"].to(device)\n",
        "\n",
        "        max_cmds = len(cmds)\n",
        "        # reshape commands into (1, max_cmds, cmd_len)\n",
        "        cmd_input_ids = cmds_tok[\"input_ids\"].unsqueeze(0).to(device)\n",
        "        cmd_attention_mask = cmds_tok[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        cmds_counts = torch.tensor([max_cmds], device=device)\n",
        "\n",
        "        logits = model(b_obs_input_ids, b_obs_attention_mask, cmd_input_ids, cmd_attention_mask, cmds_counts)\n",
        "        # logits shape (1, max_cmds)\n",
        "        idx = logits.argmax(dim=1).item()\n",
        "        return admissible_commands[idx]\n"
      ],
      "metadata": {
        "id": "6wHappODAOQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ybdvWBQdFIqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FRd4pkGzFJZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zCp5yw5-g79"
      },
      "source": [
        "While not being perfect, the agent manage to score more points on average compared to the random agent."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}