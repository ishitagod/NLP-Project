{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87gdZNiA-g7h"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/microsoft/TextWorld/blob/main/notebooks/Building%20a%20simple%20agent.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iYrYE7H-g7k"
      },
      "source": [
        "# Training LLM to play games\n",
        "\n",
        "This tutorial outlines the steps to build an agent that learns how to play __choice-based__ text-based games generated with TextWorld."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi8oVNXe-g7m"
      },
      "source": [
        "### Prerequisite\n",
        "Install TextWorld as described in the [README.md](https://github.com/microsoft/TextWorld#readme). Most of the time, a simple `pip install` should work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "E4iHT5hP-g7n",
        "outputId": "0f5d0182-3d31-40b4-cb5d-c7f997efae15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textworld\n",
            "  Downloading textworld-1.6.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.12/dist-packages (from textworld) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.17.1 in /usr/local/lib/python3.12/dist-packages (from textworld) (4.67.1)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from textworld) (2.0.0)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.12/dist-packages (from textworld) (3.5)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.12/dist-packages (from textworld) (10.8.0)\n",
            "Collecting tatsu==5.8.3 (from textworld)\n",
            "  Downloading TatSu-5.8.3-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting hashids>=1.2.0 (from textworld)\n",
            "  Downloading hashids-1.3.1-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting jericho>=3.3.0 (from textworld)\n",
            "  Downloading jericho-3.3.1-py3-none-any.whl.metadata (561 bytes)\n",
            "Collecting mementos>=1.3.1 (from textworld)\n",
            "  Downloading mementos-1.3.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from textworld) (3.2.0)\n",
            "Requirement already satisfied: prompt_toolkit in /usr/local/lib/python3.12/dist-packages (from textworld) (3.0.52)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->textworld) (2.23)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from jericho>=3.3.0->textworld) (3.8.11)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit->textworld) (0.2.14)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (0.20.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho>=3.3.0->textworld) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.3.0->textworld) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.3.0->textworld) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.3.0->textworld) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.3.0->textworld) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=2.1.0->jericho>=3.3.0->textworld) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy>=2.1.0->jericho>=3.3.0->textworld) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy>=2.1.0->jericho>=3.3.0->textworld) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy>=2.1.0->jericho>=3.3.0->textworld) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy>=2.1.0->jericho>=3.3.0->textworld) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy>=2.1.0->jericho>=3.3.0->textworld) (2.0.1)\n",
            "Downloading textworld-1.6.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading TatSu-5.8.3-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hashids-1.3.1-py2.py3-none-any.whl (6.6 kB)\n",
            "Downloading jericho-3.3.1-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.8/325.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mementos-1.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Installing collected packages: mementos, tatsu, hashids, jericho, textworld\n",
            "Successfully installed hashids-1.3.1 jericho-3.3.1 mementos-1.3.1 tatsu-5.8.3 textworld-1.6.2\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym) (3.1.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym) (0.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install textworld\n",
        "!pip install gymnasium\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWMN_YCk-g7o"
      },
      "source": [
        "and [PyTorch](https://pytorch.org/) (tested with both v1.8.2 and v.1.9.0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ItrZAyMw-g7p",
        "outputId": "ff8c90d0-5457-4471-e07e-bbe370817af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extra"
      ],
      "metadata": {
        "id": "QGtSL09R_C9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OdNDfmiL_Ell",
        "outputId": "6708e68a-3b59-4bb7-bfaf-1883dd60e242"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03kOFKtv-g7p"
      },
      "source": [
        "**[Optional]** Download all data beforehand. Otherwise, they are going to be generated as needed (slower)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0TcWNQLn-g7q",
        "outputId": "456273af-c0b5-4598-a8f0-34d9ac4381c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-25 12:40:17--  https://aka.ms/textworld/notebooks/data.zip\n",
            "Resolving aka.ms (aka.ms)... 23.204.250.75\n",
            "Connecting to aka.ms (aka.ms)|23.204.250.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://textworld.blob.core.windows.net/$web/notebooks/data.zip [following]\n",
            "--2025-11-25 12:40:17--  https://textworld.blob.core.windows.net/$web/notebooks/data.zip\n",
            "Resolving textworld.blob.core.windows.net (textworld.blob.core.windows.net)... 57.150.159.97\n",
            "Connecting to textworld.blob.core.windows.net (textworld.blob.core.windows.net)|57.150.159.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 409 Public access is not permitted on this storage account.\n",
            "2025-11-25 12:40:17 ERROR 409: Public access is not permitted on this storage account..\n",
            "\n",
            "unzip:  cannot find or open data.zip, data.zip.zip or data.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!wget https://aka.ms/textworld/notebooks/data.zip\n",
        "!unzip -nq data.zip && rm -f data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84NZBiZH-g7r"
      },
      "source": [
        "## Learning challenges\n",
        "Training an agent such that it can learn how to play text-based games is not trivial. Among other challenges, we have to deal with\n",
        "\n",
        "1. a combinatorial action space (that grows w.r.t. vocabulary)\n",
        "2. a really sparse reward signal.\n",
        "\n",
        "To ease the learning process, we will be requesting additional information alongside the game's narrative (as covered in [Playing TextWorld generated games with OpenAI Gym](Playing%20TextWorld%20generated%20games%20with%20OpenAI%20Gym.ipynb#Interact-with-the-game)). More specifically, we will request the following information:\n",
        "\n",
        "- __Description__:\n",
        "For every game state, we will get the output of the `look` command which describes the current location;\n",
        "\n",
        "- __Inventory__:\n",
        "For every game state, we will get the output of the `inventory` command which describes the player's inventory;\n",
        "\n",
        "- __Admissible commands__:\n",
        "For every game state, we will get the list of commands guaranteed to be understood by the game interpreter;\n",
        "\n",
        "- __Intermediate reward__:\n",
        "For every game state, we will get an intermediate reward which can either be:\n",
        "  - __-1__: last action needs to be undone before resuming the quest\n",
        "  -  __0__: last action didn't affect the quest\n",
        "  -  __1__: last action brought us closer to completing the quest\n",
        "\n",
        "- __Entities__:\n",
        "For every game, we will get a list of entity names that the agent can interact with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwA8xfFj-g7s"
      },
      "source": [
        "## Simple test games\n",
        "We can use TextWorld to generate a few simple games with the following handcrafted world\n",
        "```\n",
        "                     Bathroom\n",
        "                        +\n",
        "                        |\n",
        "                        +\n",
        "    Bedroom +-(d1)-+ Kitchen +--(d2)--+ Backyard\n",
        "      (P)               +                  +\n",
        "                        |                  |\n",
        "                        +                  +\n",
        "                   Living Room           Garden\n",
        "```\n",
        "where the goal is always to retrieve a hidden food item and put it on the stove which is located in the kitchen. One can lose the game if it eats the food item instead of putting it on the stove!\n",
        "\n",
        "Using `tw-make tw-simple ...`, we are going to generate the following 7 games:\n",
        "\n",
        "| gamefile | description |\n",
        "| :------- | :---------- |\n",
        "| `games/rewardsDense_goalDetailed.z8` | dense reward + detailed instructions |\n",
        "| `games/rewardsBalanced_goalDetailed.z8` | balanced rewards + detailed instructions |\n",
        "| `games/rewardsSparse_goalDetailed.z8` | sparse rewards + detailed instructions |\n",
        "| | |\n",
        "| `games/rewardsDense_goalBrief.z8` | dense rewards + no instructions but the goal is mentionned |\n",
        "| `games/rewardsBalanced_goalBrief.z8` | balanced rewards + no instructions but the goal is mentionned |\n",
        "| `games/rewardsSparse_goalBrief.z8` | sparse rewards + no instructions but the goal is mentionned |\n",
        "| | |\n",
        "| `games/rewardsSparse_goalNone.z8` | sparse rewards + no instructions/goal<br>_Hint: there's an hidden note in the game that describes the goal!_ |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5aNoPJz2-g7t"
      },
      "outputs": [],
      "source": [
        "# You can skip this if you already downloaded the games in the prequisite section.\n",
        "\n",
        "# Same as !make_games.sh\n",
        "!tw-make tw-simple --rewards dense    --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsDense_goalDetailed.z8\n",
        "!tw-make tw-simple --rewards balanced --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalDetailed.z8\n",
        "!tw-make tw-simple --rewards sparse   --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalDetailed.z8\n",
        "!tw-make tw-simple --rewards dense    --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsDense_goalBrief.z8\n",
        "!tw-make tw-simple --rewards balanced --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalBrief.z8\n",
        "!tw-make tw-simple --rewards sparse   --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalBrief.z8\n",
        "!tw-make tw-simple --rewards sparse   --goal none     --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalNone.z8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTcVVCGc-g7t"
      },
      "source": [
        "## Building the random baseline\n",
        "Let's start with building an agent that simply selects an admissible command at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9cyWYxnI-g7u"
      },
      "outputs": [],
      "source": [
        "from typing import Mapping, Any\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import textworld.gym\n",
        "\n",
        "\n",
        "class RandomAgent(textworld.gym.Agent):\n",
        "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
        "    def __init__(self, seed=1234):\n",
        "        self.seed = seed\n",
        "        self.rng = np.random.RandomState(self.seed)\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> textworld.EnvInfos:\n",
        "        return textworld.EnvInfos(admissible_commands=True)\n",
        "\n",
        "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
        "        return self.rng.choice(infos[\"admissible_commands\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttwR0osd-g7u"
      },
      "source": [
        "## Play function\n",
        "Let's write a simple play function that we will use to evaluate our agent on a given game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aJwu1_Kh-g7u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import textworld.gym\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def play(agent, path, max_step=100, nb_episodes=10, verbose=True):\n",
        "    torch.manual_seed(20211021)  # For reproducibility when using action sampling.\n",
        "\n",
        "    infos_to_request = agent.infos_to_request\n",
        "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
        "\n",
        "    gamefiles = [path]\n",
        "    if os.path.isdir(path):\n",
        "        gamefiles = glob(os.path.join(path, \"*.z8\"))\n",
        "\n",
        "    env_id = textworld.gym.register_games(gamefiles,\n",
        "                                          request_infos=infos_to_request,\n",
        "                                          max_episode_steps=max_step)\n",
        "    env = textworld.gym.make(env_id)  # Create a Gym environment to play the text game.\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            print(os.path.dirname(path), end=\"\")\n",
        "        else:\n",
        "            print(os.path.basename(path), end=\"\")\n",
        "\n",
        "    # Collect some statistics: nb_steps, final reward.\n",
        "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
        "    for no_episode in range(nb_episodes):\n",
        "        obs, infos = env.reset()  # Start new episode.\n",
        "\n",
        "        score = 0\n",
        "        done = False\n",
        "        nb_moves = 0\n",
        "        while not done:\n",
        "            command = agent.act(obs, score, done, infos)\n",
        "            obs, score, done, infos = env.step(command)\n",
        "            nb_moves += 1\n",
        "\n",
        "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
        "\n",
        "        if verbose:\n",
        "            print(\".\", end=\"\")\n",
        "        avg_moves.append(nb_moves)\n",
        "        avg_scores.append(score)\n",
        "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
        "\n",
        "    env.close()\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
        "        else:\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59ZyeTym-g7v"
      },
      "source": [
        "#### Evaluate the random agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8wdvZC7-g7v",
        "outputId": "2c42dd92-992c-4f93-828b-e9109537120e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-rewardsDense_goalDetailed.z8..........  \tavg. steps: 100.0; avg. score:  4.2 / 10.\n",
            "tw-rewardsBalanced_goalDetailed.z8..........  \tavg. steps: 100.0; avg. score:  0.7 / 4.\n",
            "tw-rewardsSparse_goalDetailed.z8..........  \tavg. steps: 100.0; avg. score:  0.0 / 1.\n"
          ]
        }
      ],
      "source": [
        "# We report the score and steps averaged over 10 playthroughs.\n",
        "play(RandomAgent(), \"./games/tw-rewardsDense_goalDetailed.z8\")    # Dense rewards\n",
        "play(RandomAgent(), \"./games/tw-rewardsBalanced_goalDetailed.z8\") # Balanced rewards\n",
        "play(RandomAgent(), \"./games/tw-rewardsSparse_goalDetailed.z8\")   # Sparse rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-497NeD-g7x"
      },
      "source": [
        "## Neural agent\n",
        "\n",
        "Now, let's create an agent that can learn to play text-based games. The agent will be trained to select a command from the list of admissible commands given the current game's narrative, inventory, and room description. Here is an overview of the architecture used for the agent:\n",
        "\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/MarcCote/TextWorld/msr_summit_2021/notebooks/figs/neural_agent.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL891ihI-g7x"
      },
      "source": [
        "### Code\n",
        "Here's the implementation of that learning agent built with [PyTorch](https://pytorch.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx2defU5-g7x",
        "outputId": "bcb1c634-4958-4d38-8747-c5dac79d48b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:114: SyntaxWarning: invalid escape sequence '\\-'\n",
            "<>:114: SyntaxWarning: invalid escape sequence '\\-'\n",
            "/tmp/ipython-input-3165348506.py:114: SyntaxWarning: invalid escape sequence '\\-'\n",
            "  text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from typing import List, Mapping, Any, Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import textworld\n",
        "import textworld.gym\n",
        "from textworld import EnvInfos\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class CommandScorer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(CommandScorer, self).__init__()\n",
        "        torch.manual_seed(42)  # For reproducibility\n",
        "        self.embedding    = nn.Embedding(input_size, hidden_size)\n",
        "        self.encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
        "        self.cmd_encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
        "        self.state_gru    = nn.GRU(hidden_size, hidden_size)\n",
        "        self.hidden_size  = hidden_size\n",
        "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
        "        self.critic       = nn.Linear(hidden_size, 1)\n",
        "        self.att_cmd      = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "    def forward(self, obs, commands, **kwargs):\n",
        "        input_length = obs.size(0)\n",
        "        batch_size = obs.size(1)\n",
        "        nb_cmds = commands.size(1)\n",
        "\n",
        "        embedded = self.embedding(obs)\n",
        "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
        "        state_output, state_hidden = self.state_gru(encoder_hidden, self.state_hidden)\n",
        "        self.state_hidden = state_hidden\n",
        "        value = self.critic(state_output)\n",
        "\n",
        "        # Attention network over the commands.\n",
        "        cmds_embedding = self.embedding.forward(commands)\n",
        "        _, cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding)  # 1 x cmds x hidden\n",
        "\n",
        "        # Same observed state for all commands.\n",
        "        cmd_selector_input = torch.stack([state_hidden] * nb_cmds, 2)  # 1 x batch x cmds x hidden\n",
        "\n",
        "        # Same command choices for the whole batch.\n",
        "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states] * batch_size, 1)  # 1 x batch x cmds x hidden\n",
        "\n",
        "        # Concatenate the observed state and command encodings.\n",
        "        cmd_selector_input = torch.cat([cmd_selector_input, cmds_encoding_last_states], dim=-1)\n",
        "\n",
        "        # Compute one score per command.\n",
        "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1)  # 1 x Batch x cmds\n",
        "\n",
        "        probs = F.softmax(scores, dim=2)  # 1 x Batch x cmds\n",
        "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x indx\n",
        "        return scores, index, value\n",
        "\n",
        "    def reset_hidden(self, batch_size):\n",
        "        self.state_hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class NeuralAgent:\n",
        "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
        "    MAX_VOCAB_SIZE = 1000\n",
        "    UPDATE_FREQUENCY = 10\n",
        "    LOG_FREQUENCY = 1000\n",
        "    GAMMA = 0.9\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._initialized = False\n",
        "        self._epsiode_has_started = False\n",
        "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
        "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
        "\n",
        "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), 0.00003)\n",
        "\n",
        "        self.mode = \"test\"\n",
        "\n",
        "    def train(self):\n",
        "        self.mode = \"train\"\n",
        "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "        self.transitions = []\n",
        "        self.model.reset_hidden(1)\n",
        "        self.last_score = 0\n",
        "        self.no_train_step = 0\n",
        "\n",
        "    def test(self):\n",
        "        self.mode = \"test\"\n",
        "        self.model.reset_hidden(1)\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> EnvInfos:\n",
        "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
        "                        won=True, lost=True)\n",
        "\n",
        "    def _get_word_id(self, word):\n",
        "        if word not in self.word2id:\n",
        "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
        "                return self.word2id[\"<UNK>\"]\n",
        "\n",
        "            self.id2word.append(word)\n",
        "            self.word2id[word] = len(self.word2id)\n",
        "\n",
        "        return self.word2id[word]\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
        "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
        "        word_ids = list(map(self._get_word_id, text.split()))\n",
        "        return word_ids\n",
        "\n",
        "    def _process(self, texts):\n",
        "        texts = list(map(self._tokenize, texts))\n",
        "        max_len = max(len(l) for l in texts)\n",
        "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            padded[i, :len(text)] = text\n",
        "\n",
        "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
        "        padded_tensor = padded_tensor.permute(1, 0) # Batch x Seq => Seq x Batch\n",
        "        return padded_tensor\n",
        "\n",
        "    def _discount_rewards(self, last_values):\n",
        "        returns, advantages = [], []\n",
        "        R = last_values.data\n",
        "        for t in reversed(range(len(self.transitions))):\n",
        "            rewards, _, _, values = self.transitions[t]\n",
        "            R = rewards + self.GAMMA * R\n",
        "            adv = R - values\n",
        "            returns.append(R)\n",
        "            advantages.append(adv)\n",
        "\n",
        "        return returns[::-1], advantages[::-1]\n",
        "\n",
        "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
        "\n",
        "        # Build agent's observation: feedback + look + inventory.\n",
        "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
        "\n",
        "        # Tokenize and pad the input and the commands to chose from.\n",
        "        input_tensor = self._process([input_])\n",
        "        commands_tensor = self._process(infos[\"admissible_commands\"])\n",
        "\n",
        "        # Get our next action and value prediction.\n",
        "        outputs, indexes, values = self.model(input_tensor, commands_tensor)\n",
        "        action = infos[\"admissible_commands\"][indexes[0]]\n",
        "\n",
        "        if self.mode == \"test\":\n",
        "            if done:\n",
        "                self.model.reset_hidden(1)\n",
        "            return action\n",
        "\n",
        "        self.no_train_step += 1\n",
        "\n",
        "        if self.transitions:\n",
        "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
        "            self.last_score = score\n",
        "            if infos[\"won\"]:\n",
        "                reward += 100\n",
        "            if infos[\"lost\"]:\n",
        "                reward -= 100\n",
        "\n",
        "            self.transitions[-1][0] = reward  # Update reward information.\n",
        "\n",
        "        self.stats[\"max\"][\"score\"].append(score)\n",
        "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
        "            # Update model\n",
        "            returns, advantages = self._discount_rewards(values)\n",
        "\n",
        "            loss = 0\n",
        "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
        "                reward, indexes_, outputs_, values_ = transition\n",
        "\n",
        "                advantage        = advantage.detach() # Block gradients flow here.\n",
        "                probs            = F.softmax(outputs_, dim=2)\n",
        "                log_probs        = torch.log(probs)\n",
        "                log_action_probs = log_probs.gather(2, indexes_)\n",
        "                policy_loss      = (-log_action_probs * advantage).sum()\n",
        "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
        "                entropy     = (-probs * log_probs).sum()\n",
        "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
        "\n",
        "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
        "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
        "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
        "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
        "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
        "\n",
        "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
        "                msg = \"{:6d}. \".format(self.no_train_step)\n",
        "                msg += \"  \".join(\"{}: {: 3.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
        "                msg += \"  \" + \"  \".join(\"{}: {:2d}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
        "                msg += \"  vocab: {:3d}\".format(len(self.id2word))\n",
        "                print(msg)\n",
        "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            self.transitions = []\n",
        "            self.model.reset_hidden(1)\n",
        "        else:\n",
        "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
        "            self.transitions.append([None, indexes, outputs, values])  # Reward will be set on the next call\n",
        "\n",
        "        if done:\n",
        "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
        "\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adNG96Ig-g7z"
      },
      "source": [
        "### Training the neural agent\n",
        "Let's first evaluate the agent before training to get a sense of its initial performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj14oGle-g7z",
        "outputId": "171f442a-80d4-4e6c-9bd4-f31d92217f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-rewardsDense_goalDetailed.z8..........  \tavg. steps: 100.0; avg. score:  5.0 / 10.\n"
          ]
        }
      ],
      "source": [
        "agent = NeuralAgent()\n",
        "agent.model.to(device) # Ensure the model is on the correct device\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLZF0Cwt-g70"
      },
      "source": [
        "Unsurprisingly, the result is not much different from what the random agent can achieve since our neural agent is initialized to a random policy.\n",
        "\n",
        "Let's train the agent for a few episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8AYQpGb-g70",
        "outputId": "cfddc34f-57d2-4baf-ccd9-8d9caf33d762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "  1000. reward:  0.056  policy:  0.352  value:  0.095  entropy:  2.468  confidence:  0.089  score:  9  vocab: 301\n",
            "  2000. reward:  0.040  policy:  0.082  value:  0.069  entropy:  2.333  confidence:  0.099  score:  6  vocab: 301\n",
            "  3000. reward:  0.054  policy:  0.173  value:  0.092  entropy:  2.395  confidence:  0.095  score:  7  vocab: 301\n",
            "  4000. reward:  0.041  policy: -0.022  value:  0.072  entropy:  2.363  confidence:  0.098  score:  6  vocab: 301\n",
            "  5000. reward:  0.053  policy:  0.038  value:  0.098  entropy:  2.405  confidence:  0.096  score:  8  vocab: 301\n",
            "  6000. reward:  0.048  policy:  0.055  value:  0.087  entropy:  2.388  confidence:  0.099  score:  7  vocab: 301\n",
            "  7000. reward:  0.047  policy: -0.051  value:  0.066  entropy:  2.338  confidence:  0.104  score:  8  vocab: 301\n",
            "  8000. reward:  0.052  policy:  0.084  value:  0.090  entropy:  2.425  confidence:  0.097  score:  7  vocab: 301\n",
            "  9000. reward:  0.050  policy:  0.002  value:  0.074  entropy:  2.364  confidence:  0.103  score:  6  vocab: 301\n",
            " 10000. reward:  0.061  policy:  0.065  value:  0.096  entropy:  2.441  confidence:  0.098  score:  9  vocab: 301\n",
            " 11000. reward:  0.051  policy:  0.029  value:  0.095  entropy:  2.428  confidence:  0.098  score:  7  vocab: 301\n",
            " 12000. reward:  0.053  policy:  0.000  value:  0.102  entropy:  2.439  confidence:  0.097  score:  9  vocab: 302\n",
            " 13000. reward:  0.058  policy:  0.015  value:  0.094  entropy:  2.477  confidence:  0.093  score:  7  vocab: 302\n",
            " 14000. reward: -0.047  policy: -1.195  value:  20.762  entropy:  2.400  confidence:  0.100  score:  9  vocab: 319\n",
            " 15000. reward: -0.043  policy: -1.285  value:  19.142  entropy:  2.496  confidence:  0.095  score:  9  vocab: 320\n",
            " 16000. reward: -0.050  policy: -0.681  value:  10.132  entropy:  2.427  confidence:  0.100  score:  9  vocab: 321\n",
            " 17000. reward:  0.069  policy:  0.057  value:  0.143  entropy:  2.437  confidence:  0.100  score:  8  vocab: 321\n",
            " 18000. reward:  0.062  policy: -0.043  value:  0.113  entropy:  2.430  confidence:  0.101  score:  9  vocab: 321\n",
            " 19000. reward:  0.067  policy:  0.018  value:  0.141  entropy:  2.469  confidence:  0.099  score:  9  vocab: 321\n",
            " 20000. reward: -0.148  policy: -1.933  value:  29.999  entropy:  2.366  confidence:  0.109  score:  9  vocab: 323\n",
            " 21000. reward: -0.353  policy: -4.877  value:  82.068  entropy:  2.358  confidence:  0.110  score:  9  vocab: 327\n",
            " 22000. reward: -0.127  policy: -2.332  value:  41.054  entropy:  2.440  confidence:  0.107  score:  9  vocab: 329\n",
            " 23000. reward: -0.001  policy: -0.799  value:  58.859  entropy:  2.384  confidence:  0.118  score: 10  vocab: 332\n",
            " 24000. reward: -0.124  policy: -1.665  value:  28.087  entropy:  2.383  confidence:  0.120  score:  9  vocab: 333\n",
            " 25000. reward: -0.337  policy: -4.663  value:  77.093  entropy:  2.339  confidence:  0.133  score:  9  vocab: 335\n",
            " 26000. reward: -0.750  policy: -7.489  value:  132.029  entropy:  2.236  confidence:  0.160  score:  9  vocab: 340\n",
            " 27000. reward: -0.858  policy: -7.315  value:  124.062  entropy:  2.214  confidence:  0.164  score:  9  vocab: 345\n",
            " 28000. reward: -0.547  policy: -4.029  value:  73.638  entropy:  2.159  confidence:  0.175  score:  9  vocab: 347\n",
            " 29000. reward: -0.440  policy: -4.144  value:  74.201  entropy:  2.258  confidence:  0.161  score:  9  vocab: 351\n",
            " 30000. reward: -0.232  policy: -3.379  value:  54.920  entropy:  2.290  confidence:  0.153  score:  9  vocab: 353\n",
            " 31000. reward: -0.308  policy: -3.977  value:  64.130  entropy:  2.218  confidence:  0.179  score:  9  vocab: 359\n",
            " 32000. reward: -0.431  policy: -5.821  value:  93.589  entropy:  2.222  confidence:  0.173  score:  9  vocab: 361\n",
            " 33000. reward: -0.329  policy: -6.137  value:  88.906  entropy:  2.327  confidence:  0.154  score:  9  vocab: 363\n",
            " 34000. reward: -0.536  policy: -7.768  value:  114.037  entropy:  2.218  confidence:  0.171  score:  9  vocab: 367\n",
            " 35000. reward: -0.437  policy: -3.661  value:  63.767  entropy:  2.192  confidence:  0.170  score:  9  vocab: 370\n",
            " 36000. reward: -0.327  policy: -3.195  value:  54.416  entropy:  2.233  confidence:  0.162  score:  9  vocab: 371\n",
            " 37000. reward: -0.316  policy: -4.073  value:  71.230  entropy:  2.271  confidence:  0.164  score:  9  vocab: 374\n",
            " 38000. reward: -0.217  policy: -1.411  value:  27.920  entropy:  2.247  confidence:  0.168  score:  9  vocab: 375\n",
            " 39000. reward: -0.010  policy: -1.306  value:  21.018  entropy:  2.194  confidence:  0.175  score:  9  vocab: 376\n",
            " 40000. reward: -0.328  policy: -3.741  value:  60.367  entropy:  2.292  confidence:  0.163  score:  9  vocab: 377\n",
            " 41000. reward: -0.121  policy: -1.665  value:  31.227  entropy:  2.245  confidence:  0.160  score:  9  vocab: 378\n",
            " 42000. reward: -0.224  policy: -2.229  value:  37.653  entropy:  2.227  confidence:  0.173  score:  9  vocab: 379\n",
            " 43000. reward: -0.114  policy: -0.764  value:  14.787  entropy:  2.216  confidence:  0.167  score:  9  vocab: 379\n",
            " 44000. reward: -0.549  policy: -7.769  value:  117.195  entropy:  2.275  confidence:  0.159  score:  9  vocab: 381\n",
            " 45000. reward: -0.217  policy: -3.331  value:  101.719  entropy:  2.253  confidence:  0.161  score: 10  vocab: 383\n",
            "Trained in 976.78 secs\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "agent = NeuralAgent()\n",
        "agent.model.to(device)\n",
        "\n",
        "print(\"Training\")\n",
        "agent.train()  # Tell the agent it should update its parameters.\n",
        "starttime = time()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\", nb_episodes=500, verbose=False)  # Dense rewards game.\n",
        "\n",
        "print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
        "\n",
        "# Save the trained agent.\n",
        "import os\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "torch.save(agent, 'checkpoints/agent_trained_on_single_game.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVWcX02u-g71"
      },
      "source": [
        "#### Testing the agent trained on a single game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JWhxtva-g71",
        "outputId": "75448f07-4d81-4b83-d5a9-37edd5f6ead4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-rewardsDense_goalDetailed.z8..........  \tavg. steps:  77.7; avg. score:  8.7 / 10.\n"
          ]
        }
      ],
      "source": [
        "# We report the score and steps averaged over 10 playthroughs.\n",
        "agent = torch.load('checkpoints/agent_trained_on_single_game.pt',weights_only=False)\n",
        "agent.test()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Dense rewards game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NgN3uYr-g72"
      },
      "source": [
        "Of course, since we trained on that single simple game, it's not surprinsing the agent can achieve a high score on it. It would be more interesting to evaluate the generalization capability of the agent.\n",
        "\n",
        "To do so, we are going to test the agent on another game drawn from the same game distribution (i.e. same world but the goal is to pick another food item). Let's generate `games/another_game.z8` with the same rewards density (`--rewards dense`) and the same goal description (`--goal detailed`), but using `--seed 1` and without the `--test` flag (to make sure the game is not part of the test set since `games/rewardsDense_goalDetailed.z8` is)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBZfWVKg-g72",
        "outputId": "d3ff86d3-ff80-4333-ef3c-40b7a03cc175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed: 1\n",
            "Game generated: /content/games/tw-another_game.z8\n",
            "\n",
            "Objective:\n",
            "Hey, thanks for coming over to the TextWorld today, there is something I need you to do for me. First of all, you could, like, look and see that the antique trunk inside the bedroom is opened. Then, recover the old key from the antique trunk. Then, make absolutely sure that the wooden door inside the bedroom is unlocked. After unlocking the wooden door, open the wooden door in the bedroom. Then, try to head east. After that, try to travel south. Once you get through with that, take the milk from the couch within the living room. Having taken the milk, attempt to travel north. That done, rest the milk on the stove inside the kitchen. And if you do that, you're the winner!\n",
            "\n",
            "Walkthrough:\n",
            "open antique trunk > take old key from antique trunk > unlock wooden door with old key > open wooden door > go east > go south > take milk from couch > go north > put milk on stove\n",
            "\n",
            "-= Stats =-\n",
            "Nb. locations: 6\n",
            "Nb. objects: 28\n"
          ]
        }
      ],
      "source": [
        "!tw-make tw-simple --rewards dense --goal detailed --seed 1 --output games/tw-another_game.z8 -v -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eChciJY_-g73",
        "outputId": "062b4a82-3271-4254-a875-319983abfc0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-another_game.z8..........  \tavg. steps: 100.0; avg. score:  3.9 / 8.\n",
            "tw-another_game.z8..........  \tavg. steps: 100.0; avg. score:  5.6 / 8.\n"
          ]
        }
      ],
      "source": [
        "# We report the score and steps averaged over 10 playthroughs.\n",
        "play(RandomAgent(), \"./games/tw-another_game.z8\")\n",
        "play(agent, \"./games/tw-another_game.z8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZX8YhBA-g73"
      },
      "source": [
        "As we can see the trained agent does a bit better than the random agent. In order to improve the agent's generalization capability, we should train it on many different games drawn from the game distribution.\n",
        "\n",
        "One could use the following command to easily generate 100 training games:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rL_r42_-g73",
        "outputId": "5c14b903-aa39-4a65-c2e7-9337d19fa186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed: 1\n",
            "Global seed: 2\n",
            "Global seed: 3\n",
            "Global seed: 4\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-ek06H8B7uqoYFVEy.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-E5eLHkaXFk6BSgR1.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-ekDZtbGXIbO5FKp8.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-D8gMTlO8cPoEtgZx.z8\n",
            "Global seed: 6\n",
            "Global seed: 7\n",
            "Global seed: 5\n",
            "Global seed: 8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-7KpYUDDdckE0cBqZ.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-KJODI168SvJVFM9x.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-o2RVTmrEi6R5T3p0.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-68kvf8x7TBd9Iq0P.z8\n",
            "Global seed: 9\n",
            "Global seed: 10\n",
            "Global seed: 11\n",
            "Global seed: 12\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-NPQ8TkJ9i2x6fYDM.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-redEHVr6CmKYhrJg.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Qbq3h3VWFkPdtogB.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Q9nDu630U5j3tqBG.z8\n",
            "Global seed: 13\n",
            "Global seed: 14\n",
            "Global seed: 15\n",
            "Global seed: 16\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-7yGrcV9pTE8DF75n.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-jROVIEqEIya6Tr0L.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1QKVfg6YhRb1ul1e.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6GMVtjVYF5QRupyN.z8\n",
            "Global seed: 17\n",
            "Global seed: 18\n",
            "Global seed: 19\n",
            "Global seed: 20\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-nrEoHEqgUba7U1nx.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Mn8oTkr2fvv8TX1.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-VLpEiW2msKJpTZ7J.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-o8WVtobyuRR0Uqv5.z8\n",
            "Global seed: 21\n",
            "Global seed: 22\n",
            "Global seed: 23\n",
            "Global seed: 24\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-JGJdTBvVHrpBiVy5.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-M7MmCGG5i6kES1kV.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6GZ3CqJvCX9rfev3.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1MB8fmosEL9HNEv.z8\n",
            "Global seed: 25\n",
            "Global seed: 26\n",
            "Global seed: 27\n",
            "Global seed: 28\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-mn3JuMrnfPNNI5K5.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-P6RdC912uMrbcXBX.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-pGedtxVJsYDxsGaV.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-R2bxU9dJUK0LCxk0.z8\n",
            "Global seed: 29\n",
            "Global seed: 30\n",
            "Global seed: 31\n",
            "Global seed: 32\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6WNBiZyofr8mu6MG.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-W1qJibX5FqLRS3kL.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-eDi7Z2iEJ7FdL9.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-QlDlS6DxCMnVu81D.z8\n",
            "Global seed: 33\n",
            "Global seed: 34\n",
            "Global seed: 35\n",
            "Global seed: 36\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-5rjrFkZEiyOIj1y.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-dZ2oU2KnflPksnEY.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-KrNpTrdMtdqVUKOB.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-J9ylcQmmc190Cgn3.z8\n",
            "Global seed: 37\n",
            "Global seed: 38\n",
            "Global seed: 39\n",
            "Global seed: 40\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-M8xnUOBkulX7HNQX.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-535aCDxgu5MqF7R1.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-nOVQCNlrINVxIDje.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-mDjNimx7S5a5tmZ7.z8\n",
            "Global seed: 41\n",
            "Global seed: 42\n",
            "Global seed: 43\n",
            "Global seed: 44\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-566vUvgjfXgU9GK.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-67XKC3EyH2riOk8.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-El9oUXJWIYVgF1jy.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-mbJLuQBoCbbkUv1n.z8\n",
            "Global seed: 45\n",
            "Global seed: 46\n",
            "Global seed: 47\n",
            "Global seed: 48\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-M2vecROLFVBxHBvl.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-JBP7TQZ6fV7biZ9q.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-mNB2tM2ohGmRIB2J.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-aernSYY1f7rrflvB.z8\n",
            "Global seed: 49\n",
            "Global seed: 50\n",
            "Global seed: 51\n",
            "Global seed: 52\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-5VVVSK6fGjlspnj.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Oq9ns8K0uK5EhJvr.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Mj8NTxYWso7LfmN9.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-xqa9SlmxsDmVs8yb.z8\n",
            "Global seed: 53\n",
            "Global seed: 54\n",
            "Global seed: 55\n",
            "Global seed: 56\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-GN1YHrQ6s6vBTnNb.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-EloyS7BZs8LRHNVn.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-XR5RUnmMIgOnCYyy.z8\n",
            "Global seed: 57\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-p1RLFX7MU7KjFy0d.z8\n",
            "Global seed: 58\n",
            "Global seed: 59\n",
            "Global seed: 60\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-WxnkhG07upKRhbNV.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Kx32iybbtDGRFQl6.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-DWvMiBQvCR5sNkx.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-R7y2UJkBUeEqS8Bk.z8\n",
            "Global seed: 61\n",
            "Global seed: 62\n",
            "Global seed: 63\n",
            "Global seed: 64\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-YnvLs6vYIWvWC51e.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1EQPheDOiVB8I7m5.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-G6a7HeZPcJNGhLQ2.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6VXXCVGKiEWkhPer.z8\n",
            "Global seed: 65\n",
            "Global seed: 66\n",
            "Global seed: 68\n",
            "Global seed: 67\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1jr7FeK9TgZCQ9n.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-0a72tMGVtQnPSvMR.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1vbrCekqT6xMcMdP.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-2OOjSlaNUBKcgLy.z8\n",
            "Global seed: 69\n",
            "Global seed: 70\n",
            "Global seed: 71\n",
            "Global seed: 72\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6kRniM7gfXQNCyj6.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-kGDLudo8fXeJH1Yg.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-aMq9U18aFMlOHVEe.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-RvQYhbdKfMyqS5Wp.z8\n",
            "Global seed: 73\n",
            "Global seed: 75\n",
            "Global seed: 74\n",
            "Global seed: 76\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-8koWh6KqcgNPI8Ox.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-Eykru3m8ilOGiBnx.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-165ai83atZWatMRa.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-vKm0IdN8c0VRfJl1.z8\n",
            "Global seed: 77\n",
            "Global seed: 78\n",
            "Global seed: 79\n",
            "Global seed: 80\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-D3PEFjpNIQykuqra.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-gPyQi5vkhdQOUV0J.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-jYPJukgls6oZf6qG.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-eRNBC9q7tr5Vi3Mv.z8\n",
            "Global seed: 81\n",
            "Global seed: 82\n",
            "Global seed: 83\n",
            "Global seed: 84\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-0lvotxEKtYZ6umyJ.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-3bj9creDs3V0IKKN.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-qZDfWDNTJmQiLvq.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-6yPJsEJMTeR6fpPR.z8\n",
            "Global seed: 85\n",
            "Global seed: 86\n",
            "Global seed: 87\n",
            "Global seed: 88\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-WrmvTdgGc05rh3eZ.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-g0GEcMQ5CWaeF3nJ.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-5PBeh9J3IX5XUjL3.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-DvPBs6OphW6nCnQ3.z8\n",
            "Global seed: 89\n",
            "Global seed: 90\n",
            "Global seed: 91\n",
            "Global seed: 92\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-XElPhYRjT8gWs1eM.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-eBxGu3KeiYjbHDl9.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-3qLvIOZ9c66Igby.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-OWRZsvM8h6XyIpQy.z8\n",
            "Global seed: 93\n",
            "Global seed: 94\n",
            "Global seed: 95\n",
            "Global seed: 96\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-gkdXTMLPtKpeSY5J.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-n3OYtnV7IY8rTkBq.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-q3rdiVK5I1lrsNpj.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-WvgBsoOmhDObfRgL.z8\n",
            "Global seed: 97\n",
            "Global seed: 98\n",
            "Global seed: 99\n",
            "Global seed: 100\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-xQNoFqlDs10nfr79.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-1dgPTd1ki5kQUkrn.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-o7PNc5j0URa5HYLb.z8\n",
            "Game generated: /content/training_games/tw-simple-rDense+gDetailed+train-house-GP-2KK5iXD8ueb3ikl6.z8\n"
          ]
        }
      ],
      "source": [
        "# You can skip this if you already downloaded the data in the prequisite section.\n",
        "\n",
        "! seq 1 100 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --format z8 --output training_games/ --seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZaRUNfp-g74"
      },
      "source": [
        "Then, we train our agent on that set of training games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC64MOKm-g75",
        "outputId": "5a2f553d-83f0-4c60-8cb6-7a0c61027a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 100 games\n",
            "  1000. reward:  0.050  policy:  0.398  value:  0.102  entropy:  2.377  confidence:  0.096  score:  9  vocab: 448\n",
            "  2000. reward:  0.051  policy:  0.238  value:  0.091  entropy:  2.361  confidence:  0.097  score:  9  vocab: 544\n",
            "  3000. reward:  0.043  policy:  0.050  value:  0.075  entropy:  2.340  confidence:  0.099  score:  7  vocab: 601\n",
            "  4000. reward:  0.042  policy:  0.031  value:  0.072  entropy:  2.352  confidence:  0.099  score:  6  vocab: 624\n",
            "  5000. reward:  0.048  policy:  0.087  value:  0.101  entropy:  2.397  confidence:  0.095  score:  5  vocab: 629\n",
            "  6000. reward:  0.054  policy:  0.093  value:  0.092  entropy:  2.420  confidence:  0.094  score:  8  vocab: 645\n",
            "  7000. reward:  0.052  policy: -0.004  value:  0.089  entropy:  2.411  confidence:  0.095  score:  7  vocab: 658\n",
            "  8000. reward:  0.051  policy:  0.033  value:  0.079  entropy:  2.401  confidence:  0.097  score:  7  vocab: 664\n",
            "  9000. reward: -0.057  policy: -1.905  value:  24.984  entropy:  2.474  confidence:  0.092  score:  7  vocab: 693\n",
            " 10000. reward: -0.051  policy: -1.614  value:  24.873  entropy:  2.400  confidence:  0.098  score:  9  vocab: 700\n",
            " 11000. reward:  0.053  policy: -0.033  value:  0.077  entropy:  2.422  confidence:  0.097  score:  7  vocab: 703\n",
            " 12000. reward:  0.056  policy:  0.047  value:  0.089  entropy:  2.466  confidence:  0.095  score:  6  vocab: 704\n",
            " 13000. reward:  0.059  policy:  0.010  value:  0.084  entropy:  2.416  confidence:  0.097  score:  7  vocab: 705\n",
            " 14000. reward:  0.056  policy:  0.027  value:  0.108  entropy:  2.487  confidence:  0.092  score:  7  vocab: 707\n",
            " 15000. reward:  0.052  policy: -0.042  value:  0.072  entropy:  2.451  confidence:  0.096  score:  6  vocab: 707\n",
            " 16000. reward:  0.058  policy: -0.017  value:  0.082  entropy:  2.429  confidence:  0.096  score:  8  vocab: 710\n",
            " 17000. reward:  0.054  policy:  0.047  value:  0.099  entropy:  2.475  confidence:  0.092  score:  6  vocab: 711\n",
            " 18000. reward: -0.056  policy: -1.054  value:  16.723  entropy:  2.436  confidence:  0.097  score:  6  vocab: 713\n",
            " 19000. reward:  0.049  policy: -0.061  value:  0.074  entropy:  2.426  confidence:  0.098  score:  6  vocab: 713\n",
            " 20000. reward:  0.048  policy: -0.071  value:  0.097  entropy:  2.457  confidence:  0.099  score:  5  vocab: 713\n",
            " 21000. reward:  0.059  policy:  0.043  value:  0.092  entropy:  2.382  confidence:  0.104  score:  6  vocab: 713\n",
            " 22000. reward:  0.059  policy:  0.024  value:  0.094  entropy:  2.415  confidence:  0.100  score:  8  vocab: 713\n",
            " 23000. reward:  0.050  policy: -0.048  value:  0.096  entropy:  2.421  confidence:  0.103  score:  6  vocab: 713\n",
            " 24000. reward:  0.057  policy:  0.010  value:  0.091  entropy:  2.467  confidence:  0.095  score:  6  vocab: 713\n",
            " 25000. reward:  0.059  policy: -0.031  value:  0.074  entropy:  2.468  confidence:  0.096  score:  6  vocab: 713\n",
            " 26000. reward:  0.057  policy: -0.028  value:  0.086  entropy:  2.439  confidence:  0.098  score:  7  vocab: 713\n",
            " 27000. reward:  0.060  policy:  0.040  value:  0.124  entropy:  2.449  confidence:  0.099  score:  9  vocab: 713\n",
            " 28000. reward:  0.060  policy: -0.018  value:  0.102  entropy:  2.450  confidence:  0.102  score:  9  vocab: 713\n",
            " 29000. reward: -0.050  policy: -1.177  value:  18.636  entropy:  2.410  confidence:  0.115  score:  9  vocab: 714\n",
            " 30000. reward:  0.069  policy:  0.003  value:  0.122  entropy:  2.352  confidence:  0.111  score:  8  vocab: 714\n",
            " 31000. reward: -0.040  policy: -0.743  value:  13.795  entropy:  2.452  confidence:  0.102  score:  9  vocab: 716\n",
            " 32000. reward: -0.246  policy: -3.125  value:  48.537  entropy:  2.497  confidence:  0.107  score:  9  vocab: 722\n",
            " 33000. reward:  0.068  policy: -0.012  value:  0.145  entropy:  2.506  confidence:  0.108  score:  9  vocab: 722\n",
            " 34000. reward:  0.070  policy: -0.072  value:  0.114  entropy:  2.431  confidence:  0.112  score:  9  vocab: 723\n",
            " 35000. reward:  0.073  policy:  0.049  value:  0.135  entropy:  2.452  confidence:  0.110  score:  9  vocab: 724\n",
            " 36000. reward:  0.067  policy: -0.001  value:  0.155  entropy:  2.548  confidence:  0.101  score:  9  vocab: 724\n",
            " 37000. reward: -0.034  policy: -1.326  value:  21.685  entropy:  2.440  confidence:  0.112  score:  9  vocab: 725\n",
            " 38000. reward: -0.253  policy: -3.575  value:  55.042  entropy:  2.444  confidence:  0.113  score:  9  vocab: 729\n",
            " 39000. reward: -0.026  policy: -2.089  value:  58.135  entropy:  2.392  confidence:  0.127  score:  9  vocab: 733\n",
            " 40000. reward: -0.034  policy: -0.628  value:  13.535  entropy:  2.340  confidence:  0.128  score:  9  vocab: 735\n",
            " 41000. reward: -0.238  policy: -2.358  value:  45.089  entropy:  2.393  confidence:  0.127  score: 10  vocab: 738\n",
            " 42000. reward: -0.131  policy: -1.446  value:  25.455  entropy:  2.393  confidence:  0.135  score:  9  vocab: 741\n",
            " 43000. reward: -0.357  policy: -4.487  value:  65.212  entropy:  2.363  confidence:  0.131  score:  9  vocab: 747\n",
            " 44000. reward: -0.022  policy: -1.616  value:  24.861  entropy:  2.338  confidence:  0.138  score:  9  vocab: 747\n",
            " 45000. reward: -0.252  policy: -3.218  value:  51.320  entropy:  2.335  confidence:  0.136  score:  9  vocab: 750\n",
            " 46000. reward: -0.242  policy: -3.650  value:  57.736  entropy:  2.341  confidence:  0.131  score:  9  vocab: 752\n",
            " 47000. reward: -0.356  policy: -4.112  value:  75.110  entropy:  2.407  confidence:  0.121  score:  9  vocab: 754\n",
            " 48000. reward: -0.037  policy: -1.061  value:  16.008  entropy:  2.374  confidence:  0.127  score:  9  vocab: 754\n",
            "Trained in 1042.89 secs\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "agent = NeuralAgent()\n",
        "agent.model.to(device)\n",
        "\n",
        "print(\"Training on 100 games\")\n",
        "agent.train()  # Tell the agent it should update its parameters.\n",
        "starttime = time()\n",
        "play(agent, \"./training_games/\", nb_episodes=100 * 5, verbose=False)  # Each game will be seen 5 times.\n",
        "print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
        "\n",
        "# Save the trained agent.\n",
        "import os\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "torch.save(agent, 'checkpoints/agent_trained_on_multiple_games.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emP9RlJN-g75"
      },
      "source": [
        "#### Testing the agent trained on 100 games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dv5tW-2-g76",
        "outputId": "fa087e44-f29c-4c63-f060-058963060608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-rewardsDense_goalDetailed.z8..........  \tavg. steps:  83.6; avg. score:  9.0 / 10.\n",
            "tw-another_game.z8..........  \tavg. steps:  95.3; avg. score:  5.8 / 8.\n"
          ]
        }
      ],
      "source": [
        "agent = torch.load('checkpoints/agent_trained_on_multiple_games.pt', weights_only = False)\n",
        "agent.test()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Averaged over 10 playthroughs.\n",
        "play(agent, \"./games/tw-another_game.z8\")               # Averaged over 10 playthroughs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ESnjRC5-g76"
      },
      "source": [
        "Compare it to the agent trained on a single game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO4sIsxx-g76",
        "outputId": "4af8c8bc-b9e7-4ac6-baf3-137d757b9512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-rewardsDense_goalDetailed.z8..........  \tavg. steps:  77.7; avg. score:  8.7 / 10.\n",
            "tw-another_game.z8..........  \tavg. steps: 100.0; avg. score:  5.6 / 8.\n"
          ]
        }
      ],
      "source": [
        "agent = torch.load('checkpoints/agent_trained_on_single_game.pt',weights_only=False)\n",
        "agent.test()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Averaged over 10 playthroughs.\n",
        "play(agent, \"./games/tw-another_game.z8\")               # Averaged over 10 playthroughs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an8sgS5u-g77"
      },
      "source": [
        "#### Evaluating the agent on a test distribution\n",
        "We will generate 20 test games and evaluate the agent on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZA10_rH-g77",
        "outputId": "ad6f763e-92aa-45a1-ec50-24c730e44d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed: 1\n",
            "Global seed: 3\n",
            "Global seed: 4\n",
            "Global seed: 2\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-ek06H8B7uqoYFVEy.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-E5eLHkaXFk6BSgR1.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-D8gMTlO8cPoEtgZx.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-ekDZtbGXIbO5FKp8.z8\n",
            "Global seed: 5\n",
            "Global seed: 6\n",
            "Global seed: 7\n",
            "Global seed: 8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-7KpYUDDdckE0cBqZ.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-o2RVTmrEi6R5T3p0.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-68kvf8x7TBd9Iq0P.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-KJODI168SvJVFM9x.z8\n",
            "Global seed: 10\n",
            "Global seed: 9\n",
            "Global seed: 11\n",
            "Global seed: 12\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-Q9nDu630U5j3tqBG.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-redEHVr6CmKYhrJg.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-NPQ8TkJ9i2x6fYDM.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-Qbq3h3VWFkPdtogB.z8\n",
            "Global seed: 13\n",
            "Global seed: 14\n",
            "Global seed: 15\n",
            "Global seed: 16\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-1QKVfg6YhRb1ul1e.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-7yGrcV9pTE8DF75n.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-jROVIEqEIya6Tr0L.z8\n",
            "Global seed: 18\n",
            "Global seed: 17\n",
            "Global seed: 19\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-6GMVtjVYF5QRupyN.z8\n",
            "Global seed: 20\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-Mn8oTkr2fvv8TX1.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-VLpEiW2msKJpTZ7J.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-nrEoHEqgUba7U1nx.z8\n",
            "Game generated: /content/testing_games/tw-simple-rDense+gDetailed+test-house-GP-o8WVtobyuRR0Uqv5.z8\n"
          ]
        }
      ],
      "source": [
        "# You can skip this if you already downloaded the games in the prequisite section.\n",
        "\n",
        "! seq 1 20 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --test --format z8 --output testing_games/ --seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmpEXkij-g78",
        "outputId": "c5d88a29-c3ea-4fd7-9dae-2c89e1b897d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./testing_games........................................................................................................................................................................................................  \tavg. steps:  99.4; avg. normalized score:  0.5 / 1.\n",
            "./testing_games........................................................................................................................................................................................................  \tavg. steps:  89.2; avg. normalized score:  0.8 / 1.\n"
          ]
        }
      ],
      "source": [
        "agent = torch.load('checkpoints/agent_trained_on_multiple_games.pt',weights_only=False)\n",
        "agent.test()\n",
        "play(RandomAgent(), \"./testing_games/\", nb_episodes=20 * 10)\n",
        "play(agent, \"./testing_games/\", nb_episodes=20 * 10)  # Averaged over 10 playthroughs for each test game."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}